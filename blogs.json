{"status":"ok","feed":{"url":"https://medium.com/feed/@hussein-awala","title":"Stories by Hussein Awala on Medium","link":"https://medium.com/@hussein-awala?source=rss-8725a0967242------2","author":"","description":"Stories by Hussein Awala on Medium","image":"https://cdn-images-1.medium.com/fit/c/150/150/1*C9ZRmXH-Bd2_7yTYum0ZEQ.jpeg"},"items":[{"title":"The Golden Path for Spark On Kubernetes","pubDate":"2024-01-29 10:16:03","link":"https://blog.stackademic.com/the-golden-path-for-spark-on-kubernetes-695b79914b59?source=rss-8725a0967242------2","guid":"https://medium.com/p/695b79914b59","author":"Hussein Awala","thumbnail":"","description":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*bETd1s8yCGgS_F7O8qlIUw.png\"></figure><p>In the realms of distributed computing and container orchestration, Apache Spark and Kubernetes are formidable leaders. But what happens when these powers\u00a0unite?</p>\n<h4>What is Apache\u00a0Spark?</h4>\n<p>Apache Spark is an open-source distributed computing system designed to handle big data processing challenges. With its in-memory computing, Spark accelerates data processing, offering speed and efficiency. Its versatile APIs in Java, Scala, Python, R, built-in libraries for machine learning, and graph processing make it accessible and adaptable.</p>\n<p>Spark has a pluggable component called the cluster manager to make it talk to any resource manager, where it utilizes the resource manager to execute the workload. Spark comes with a native resource manager, which we call standalone, but also it has a native integration with other generic cluster managers such as Mesos, YARN, and Kubernetes.</p>\n<h4>What is Kubernetes?</h4>\n<p>Kubernetes, often abbreviated as K8S, is an open-source container orchestration system originally designed by Google. It was released as open source in 2014 and is currently maintained by the CNCF. This system automates software deployment, scaling, and management, providing a robust solution for containerized applications.</p>\n<p>Kubernetes has become the go-to manager for handling clusters, making things like network management, scaling applications, load balancing, and managing resources a breeze. Its popularity skyrocketed, thanks to cloud providers offering user-friendly managed K8S services (EKS, GKE, AKS,\u00a0\u2026).</p>\n<h4>Spark On Kubernetes</h4>\n<figure><img alt=\"Spark on K8S in cluster mode\" src=\"https://cdn-images-1.medium.com/max/761/1*tkWi16Clt_6RjLJNmyLVkg.png\"><figcaption>Spark on K8S in cluster\u00a0mode</figcaption></figure><p>Apache Spark 2.3 introduced native support for submitting Spark applications on Kubernetes (<a href=\"https://issues.apache.org/jira/browse/SPARK-18278\">SPARK-18278</a>). When an application is submitted in cluster mode, Spark calls the Kubernetes API to create a pod for the driver, and then the driver takes care of creating pods for the executors. When it\u2019s submitted in client mode (from a pod in the cluster or any process outside the K8S cluster), the client will be treated as a driver and it will create the executors' pods by itself, but it should have access to the created pods to communicate with them using RPC protocol.</p>\n<p>Running Spark applications on Kubernetes makes them <strong>more scalable</strong> and <strong>less expensive</strong>, thanks to the Kubernetes node autoscaler which surpasses all the scalars of other cluster managers with its speed of scaling and provisioning of nodes. This also removes the need to pay for a Spark-managed service on the cloud (EMR, Dataproc,\u00a0\u2026).</p>\n<h3>Challenges of running Spark on Kubernetes</h3>\n<p>When the support for Kubernetes as a cluster manager for Apache Spark was introduced, many challenges were encountered by users of this integration, some of them were resolved by the Spark community, and the rest were been resolved or simplified by third-party tools. One of these third-party tools is <a href=\"https://github.com/GoogleCloudPlatform/spark-on-k8s-operator\"><strong>spark-on-k8s-operator</strong></a>.</p>\n<h4>Application logs</h4>\n<p>By default, the executors' pods are deleted when the application terminates and Apache Spark does not provide a solution for storing logs outside of pods. Fortunately, there are many FOSS tools (Fluent-bit, Fluentd,\u00a0\u2026) to collect logs from Kubernetes pods and distribute them to different destinations (Kafka, cloud watch, S3, Elasticsearch,\u00a0\u2026). For users who do not want to keep the logs and only want to debug their failed tasks, it is possible to disable automatic cleanup by adding the configuration <em>spark.kubernetes.executor.deleteOnTermination=false</em>, and manage the cleanup themselves.</p>\n<h4>Dynamic Allocation</h4>\n<p>Dynamic allocation is one of the greatest features of Apache Spark, it allows the application to <strong>acquire resources</strong> as and when they are needed and <strong>release them</strong> immediately after use, this is done by creating new executors respecting the defined limits when needed (<strong>scaling out</strong>) and adding them to the Spark context to speed up processing then removing them when they are no longer\u00a0needed.</p>\n<p>This feature presented many challenges with Kubernetes, due to the need to keep the shuffle data to avoid recomputing it when we want to scale in. One of the available solutions was to adapt one of the <strong>remote shuffle services</strong> developed for YARN to make it compatible with Kubernetes, and then configure the Spark application to store the shuffle data remotely, which avoids losing the shuffle data after removing some of the executors. This solution was not perfect because it added a new component to be managed and a new bottleneck for the Spark workload, where the performance of the Spark applications was impacted by the throughput of this\u00a0service.</p>\n<p>Later, Spark 3.0.0 introduced a new experimental dynamic allocation mechanism called <strong><em>shuffleTracking</em></strong>. It is based on tracking shuffle data and its location. The mechanism uses this information in the scaling decision by selecting executors that have no shuffled data to delete. Additionally, it defers a decision to delete an executor until its shuffle data is no longer needed. This mechanism has been considered stable since 3.3.0 (<a href=\"https://issues.apache.org/jira/browse/SPARK-39322\">SPARK-39322</a>).</p>\n<h4>Long-running applications</h4>\n<p>When we run long-running Spark applications on Kubernetes, executor pods are likely to be deleted after node scaling, or if running on spot nodes, in which case we will have the same problem we had with dynamic allocation: we will lose the shuffle data and have to recalculate it</p>\n<p>Spark 3.2 introduced a new shuffle strategy with <strong>shuffle data recovery</strong> from the PVCs (<a href=\"https://issues.apache.org/jira/browse/SPARK-35593\">SPARK-35593</a>) as a solution to this problem, but it depends on a separate feature introduced in the same release that allows Spark executors to <strong>reuse an existing PVC</strong> if there is an existing one (<a href=\"https://issues.apache.org/jira/browse/SPARK-35416\">SPARK-35416</a>).</p>\n<h4>Spark UI</h4>\n<p>The Spark application UI is essential for most Spark users, it is used to monitor the status and resource consumption of the Spark application and get the progress of the application in real-time with many useful metrics (JVM metrics, processed data information, Spark tasks, storage, and shuffle), and the status of executors. When running Spark on Kubernetes, this UI will not be accessible from outside the cluster, and to make it accessible you need to create an ingress for each Spark application service.</p>\n<p>However, the creation of this ingress could be automatized when using the spark-on-k8s-operator.</p>\n<h4>The complexity of submitting and managing the applications</h4>\n<p>The official tool for submitting Spark applications to Kubernetes (and to other cluster managers) is <strong><em>spark-submit</em></strong><em>; </em>it is a bash script in the Spark distribution that processes the provided configurations and runs a Scala application in the <strong>JVM</strong> to submit the Spark application.</p>\n<p>This tool has different problems:</p>\n<ul>\n<li>it needs <strong>JAVA</strong> installed on the\u00a0host</li>\n<li>\n<strong>Spark</strong> must be downloaded to the\u00a0host</li>\n<li>it\u2019s very generic and requires a lot of configuration</li>\n</ul>\n<p>To overcome these issues and limitations, most Spark on Kubernetes users develop their own tools or use third-party tools like spark-on-k8s-operator.</p>\n<h3>spark-on-k8s-operator</h3>\n<figure><img alt=\"Spark on K8s operator architecture\" src=\"https://cdn-images-1.medium.com/max/960/1*KsSZxnJdj32tHAHiEKHZIQ.png\"><figcaption>Spark on K8S operator architecture</figcaption></figure><p><a href=\"https://github.com/GoogleCloudPlatform/spark-on-k8s-operator\"><strong>spark-on-k8s-operator</strong></a> is a Kubernetes operator that can be installed on the cluster to extend the Kubernetes resources list (it adds some CRD like <em>SparkApplication</em>) and processes the created objects to create the spark driver pod based on the provided configurations and monitor the running applications.</p>\n<h4>Features</h4>\n<p>This operator simplifies submitting and managing the Spark jobs for its\u00a0users:</p>\n<ul>\n<li>Enables <strong>declarative</strong> application specification and management of applications through custom resources.</li>\n<li>Provides native <strong>cron</strong> support for running scheduled applications.</li>\n<li>Provides <strong><em>sparkctl</em></strong> which is a dedicated CLI for Spark to use instead of\u00a0<em>kubectl</em>.</li>\n<li>Provides a<strong> mutating admission webhook</strong> to update the driver and executor pods based on certain annotations.</li>\n</ul>\n<h4>Challenges</h4>\n<p>Some users who have a large number of Spark applications and want to reduce duplication in configurations, as well as those who want to configure their applications dynamically, both have issues with the declarative application specification, where it is difficult to create and manage it <strong>programmatically</strong>.</p>\n<p>Additionally, installing and maintaining the operator API is a prerequisite for submitting Spark applications, and this increases the likelihood of <strong>service downtime</strong>, where any failure of the operator API will block the execution of all newly submitted applications. Also, installing this type of operator requires specific <strong>permissions</strong> on the cluster, which might not be possible for some teams, especially in large companies.</p>\n<h3>Spark on K8S python\u00a0package</h3>\n<p><a href=\"https://github.com/hussein-awala/spark-on-k8s\"><strong>spark-on-k8s</strong></a> is a new project that aims to simplify submitting and managing Spark applications with Python. It communicates directly with the Kubernetes API to create the driver pod without the need to install any service on the cluster. It provides a CLI, an optional API, a web server, and integration with\u00a0Airflow.</p>\n<h4>Python client</h4>\n<p>This is a Spark client written in Python used to submit Spark applications to Kubernetes using the <a href=\"https://pypi.org/project/kubernetes/\">Kubernetes Python client</a>. This client uses two other clients internally:</p>\n<ul>\n<li>\n<strong><em>KubernetesClientManager</em></strong>: used to manage Kubernetes credentials and create the client instance.</li>\n<li>\n<strong><em>SparkAppManager</em></strong>: a class contains a list of methods used to manage existing Spark applications.</li>\n</ul>\n<p>Here is an example of submitting a Spark application with this\u00a0client:</p>\n<pre>from spark_on_k8s.client import SparkOnK8S<br><br>client = SparkOnK8S()<br>client.submit_app(<br>    image=\"my-registry/my-image:latest\",<br>    app_path=\"local:///opt/spark/work-dir/my-app.py\",<br>    app_arguments=[\"arg1\", \"arg2\"],<br>    app_name=\"my-app\",<br>    namespace=\"spark-namespace\",<br>    service_account=\"spark-service-account\",<br>    app_waiter=\"log\",<br>    image_pull_policy=\"Never\",<br>    ui_reverse_proxy=True,<br>)</pre>\n<h4>CLI</h4>\n<p>This is a wrapper for the Spark client and the application manager developed in Python using <a href=\"https://click.palletsprojects.com/\">Click</a>. The CLI default values can be configured via environment variables, allowing teams to create a\u00a0<em>.env</em> file to set their default configurations (they also work with the Python\u00a0client).</p>\n<p>Without overriding the default\u00a0values:</p>\n<pre>$ spark-on-k8s app submit --help<br>Usage: spark-on-k8s app submit [OPTIONS] [APP_ARGUMENTS]...<br><br>  Submit a Spark application.<br><br>Options:<br>  --image TEXT                    The docker image to use for the app.  [required]<br>  --path TEXT                     The path to the app to submit.  [required]<br>  -n, --namespace TEXT            The namespace to operate on.  [default: default]<br>...</pre>\n<p>After overriding some of\u00a0them:</p>\n<pre>$ SPARK_ON_K8S_DOCKER_IMAGE=default_docker_image:latest SPARK_ON_K8S_NAMESPACE=spark spark-on-k8s app submit --help<br>Usage: spark-on-k8s app submit [OPTIONS] [APP_ARGUMENTS]...<br><br>  Submit a Spark application.<br><br>Options:<br>  --image TEXT                    The docker image to use for the app.  [default: default_docker_image:latest]<br>  --path TEXT                     The path to the app to submit.  [required]<br>  -n, --namespace TEXT            The namespace to operate on.  [default: spark]<br>...</pre>\n<p>You can find the list of the supported environment variables and their description in the <a href=\"https://github.com/hussein-awala/spark-on-k8s?tab=readme-ov-file#configuration\">project documentation</a>.</p>\n<h4>API and web\u00a0server</h4>\n<p>It\u2019s an optional component that allows listing running applications and their states via a <strong>REST API</strong> or a <strong>web UI</strong>. Both packages use an async version of the application manager developed with the <a href=\"https://pypi.org/project/kubernetes-asyncio/\">kubernetes-asyncio</a> package.</p>\n<p>The web server currently has two\u00a0routes:</p>\n<p><em>&lt;URL&gt;/webserver/apps?namespace=&lt;namespace&gt;</em> to list the applications and their status in a table with the link to the Spark UI for the running applications that have the reverse proxy option activated (<strong>auto-refreshed</strong>):</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Wt8Qig-Q2mfqLw3hhaE9Pw.png\"></figure><p>And<em> &lt;URL&gt;/webserver/ui/&lt;namespace&gt;/&lt;application id&gt;</em> to access the Spark UI through the package <strong>reverse\u00a0proxy</strong>.</p>\n<p>The best way to install the API is by using the developed <a href=\"https://github.com/hussein-awala/spark-on-k8s/tree/main/chart\">helm chart</a>. You can find an example of how to use it in <a href=\"https://github.com/hussein-awala/spark-on-k8s/blob/main/README.md#api-in-production\">the documentation</a>.</p>\n<h4>Airflow</h4>\n<p>For Airflow users, the package provides an operator to submit the Spark applications to Kubernetes with all the Python client features. It works with official Kubernetes connections used by Airflow. The operators can be executed in <strong>deferrable</strong> mode to wait for the job asynchronously.</p>\n<pre>SparkOnK8SOperator(<br>    task_id=\"spark_application\",<br>    kubernetes_conn_id=\"kubernetes_default\",<br>    namespace=\"spark\",<br>    image=\"pyspark-job\",<br>    image_pull_policy=\"Never\",<br>    app_path=\"local:///opt/spark/work-dir/job.py\",<br>    app_arguments=[\"100000\"],<br>    app_name=\"pyspark-job-example\",<br>    service_account=\"spark\",<br>    app_waiter=\"log\",<br>    driver_resources=PodResources(cpu=1, memory=1024, memory_overhead=512),<br>    executor_resources=PodResources(cpu=1, memory=1024, memory_overhead=512),<br>    executor_instances=ExecutorInstances(min=0, max=5, initial=5),<br>    ui_reverse_proxy=True,<br>    deferrable=True,<br>)</pre>\n<h4>Install the\u00a0package</h4>\n<p>The package is available on PyPI, you can run this command to install\u00a0it:</p>\n<pre># Install the core pacakge<br>pip install \"spark-on-k8s\"<br><br># Install it with the API extra<br>pip install \"spark-on-k8s[api]\"<br><br># Install it with Airflow extra<br>pip install \"spark-on-k8s[airflow]\"<br><br># Install it with all the extras<br>pip install \"spark-on-k8s[api,airflow]\"</pre>\n<p>You can also use the docker image that contains the core package and the API\u00a0extra:</p>\n<pre># pull from ghcr.io<br>docker pull ghcr.io/hussein-awala/spark-on-k8s:&lt;version&gt;<br><br># or from docker hub<br>docker pull husseinawala/spark-on-k8s:&lt;version&gt;</pre>\n<h4>What\u2019s next</h4>\n<p>The package is currently in <strong>active development</strong>, and many new features and improvements will be added in future releases, such as support for creating ephemeral secrets to store and use application secrets, integration with cloud providers' services (S3, GCS, Glue,\u00a0\u2026), and new features in the API and web server to make them an alternative solution to the CLI. You can check the projects\u2019 <a href=\"https://github.com/hussein-awala/spark-on-k8s/blob/main/TODO.md\">TODO list</a> for more\u00a0details.</p>\n<h3>Summary</h3>\n<p>Although running Spark on Kubernetes makes it more powerful, cheaper, and more flexible, managing Spark applications on Kubernetes is a significant challenge for many users. Users rely on some third-party tools to simplify the management of their applications, such as spark-on-k8s-operator, but these tools do not solve all their problems and in some cases introduce new challenges. This blog introduces a new Python package that can help overcome some of these issues and provides new features to simplify the management of Spark applications, with a promising roadmap.</p>\n<h3>Stackademic \ud83c\udf93</h3>\n<p>Thank you for reading until the end. Before you\u00a0go:</p>\n<ul>\n<li>Please consider <strong>clapping</strong> and <strong>following</strong> the writer!\u00a0\ud83d\udc4f</li>\n<li>Follow us <a href=\"https://twitter.com/stackademichq\"><strong>X</strong></a> | <a href=\"https://www.linkedin.com/company/stackademic\"><strong>LinkedIn</strong></a> | <a href=\"https://www.youtube.com/c/stackademic\"><strong>YouTube</strong></a> |\u00a0<a href=\"https://discord.gg/in-plain-english-709094664682340443\"><strong>Discord</strong></a>\n</li>\n<li>Visit our other platforms: <a href=\"https://plainenglish.io/\"><strong>In Plain English</strong></a> | <a href=\"https://cofeed.app/\"><strong>CoFeed</strong></a> |\u00a0<a href=\"https://differ.blog/\"><strong>Differ</strong></a>\n</li>\n<li>More content at <a href=\"https://stackademic.com/\"><strong>Stackademic.com</strong></a>\n</li>\n</ul>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=695b79914b59\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://blog.stackademic.com/the-golden-path-for-spark-on-kubernetes-695b79914b59\">The Golden Path for Spark On Kubernetes</a> was originally published in <a href=\"https://blog.stackademic.com/\">Stackademic</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","content":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*bETd1s8yCGgS_F7O8qlIUw.png\"></figure><p>In the realms of distributed computing and container orchestration, Apache Spark and Kubernetes are formidable leaders. But what happens when these powers\u00a0unite?</p>\n<h4>What is Apache\u00a0Spark?</h4>\n<p>Apache Spark is an open-source distributed computing system designed to handle big data processing challenges. With its in-memory computing, Spark accelerates data processing, offering speed and efficiency. Its versatile APIs in Java, Scala, Python, R, built-in libraries for machine learning, and graph processing make it accessible and adaptable.</p>\n<p>Spark has a pluggable component called the cluster manager to make it talk to any resource manager, where it utilizes the resource manager to execute the workload. Spark comes with a native resource manager, which we call standalone, but also it has a native integration with other generic cluster managers such as Mesos, YARN, and Kubernetes.</p>\n<h4>What is Kubernetes?</h4>\n<p>Kubernetes, often abbreviated as K8S, is an open-source container orchestration system originally designed by Google. It was released as open source in 2014 and is currently maintained by the CNCF. This system automates software deployment, scaling, and management, providing a robust solution for containerized applications.</p>\n<p>Kubernetes has become the go-to manager for handling clusters, making things like network management, scaling applications, load balancing, and managing resources a breeze. Its popularity skyrocketed, thanks to cloud providers offering user-friendly managed K8S services (EKS, GKE, AKS,\u00a0\u2026).</p>\n<h4>Spark On Kubernetes</h4>\n<figure><img alt=\"Spark on K8S in cluster mode\" src=\"https://cdn-images-1.medium.com/max/761/1*tkWi16Clt_6RjLJNmyLVkg.png\"><figcaption>Spark on K8S in cluster\u00a0mode</figcaption></figure><p>Apache Spark 2.3 introduced native support for submitting Spark applications on Kubernetes (<a href=\"https://issues.apache.org/jira/browse/SPARK-18278\">SPARK-18278</a>). When an application is submitted in cluster mode, Spark calls the Kubernetes API to create a pod for the driver, and then the driver takes care of creating pods for the executors. When it\u2019s submitted in client mode (from a pod in the cluster or any process outside the K8S cluster), the client will be treated as a driver and it will create the executors' pods by itself, but it should have access to the created pods to communicate with them using RPC protocol.</p>\n<p>Running Spark applications on Kubernetes makes them <strong>more scalable</strong> and <strong>less expensive</strong>, thanks to the Kubernetes node autoscaler which surpasses all the scalars of other cluster managers with its speed of scaling and provisioning of nodes. This also removes the need to pay for a Spark-managed service on the cloud (EMR, Dataproc,\u00a0\u2026).</p>\n<h3>Challenges of running Spark on Kubernetes</h3>\n<p>When the support for Kubernetes as a cluster manager for Apache Spark was introduced, many challenges were encountered by users of this integration, some of them were resolved by the Spark community, and the rest were been resolved or simplified by third-party tools. One of these third-party tools is <a href=\"https://github.com/GoogleCloudPlatform/spark-on-k8s-operator\"><strong>spark-on-k8s-operator</strong></a>.</p>\n<h4>Application logs</h4>\n<p>By default, the executors' pods are deleted when the application terminates and Apache Spark does not provide a solution for storing logs outside of pods. Fortunately, there are many FOSS tools (Fluent-bit, Fluentd,\u00a0\u2026) to collect logs from Kubernetes pods and distribute them to different destinations (Kafka, cloud watch, S3, Elasticsearch,\u00a0\u2026). For users who do not want to keep the logs and only want to debug their failed tasks, it is possible to disable automatic cleanup by adding the configuration <em>spark.kubernetes.executor.deleteOnTermination=false</em>, and manage the cleanup themselves.</p>\n<h4>Dynamic Allocation</h4>\n<p>Dynamic allocation is one of the greatest features of Apache Spark, it allows the application to <strong>acquire resources</strong> as and when they are needed and <strong>release them</strong> immediately after use, this is done by creating new executors respecting the defined limits when needed (<strong>scaling out</strong>) and adding them to the Spark context to speed up processing then removing them when they are no longer\u00a0needed.</p>\n<p>This feature presented many challenges with Kubernetes, due to the need to keep the shuffle data to avoid recomputing it when we want to scale in. One of the available solutions was to adapt one of the <strong>remote shuffle services</strong> developed for YARN to make it compatible with Kubernetes, and then configure the Spark application to store the shuffle data remotely, which avoids losing the shuffle data after removing some of the executors. This solution was not perfect because it added a new component to be managed and a new bottleneck for the Spark workload, where the performance of the Spark applications was impacted by the throughput of this\u00a0service.</p>\n<p>Later, Spark 3.0.0 introduced a new experimental dynamic allocation mechanism called <strong><em>shuffleTracking</em></strong>. It is based on tracking shuffle data and its location. The mechanism uses this information in the scaling decision by selecting executors that have no shuffled data to delete. Additionally, it defers a decision to delete an executor until its shuffle data is no longer needed. This mechanism has been considered stable since 3.3.0 (<a href=\"https://issues.apache.org/jira/browse/SPARK-39322\">SPARK-39322</a>).</p>\n<h4>Long-running applications</h4>\n<p>When we run long-running Spark applications on Kubernetes, executor pods are likely to be deleted after node scaling, or if running on spot nodes, in which case we will have the same problem we had with dynamic allocation: we will lose the shuffle data and have to recalculate it</p>\n<p>Spark 3.2 introduced a new shuffle strategy with <strong>shuffle data recovery</strong> from the PVCs (<a href=\"https://issues.apache.org/jira/browse/SPARK-35593\">SPARK-35593</a>) as a solution to this problem, but it depends on a separate feature introduced in the same release that allows Spark executors to <strong>reuse an existing PVC</strong> if there is an existing one (<a href=\"https://issues.apache.org/jira/browse/SPARK-35416\">SPARK-35416</a>).</p>\n<h4>Spark UI</h4>\n<p>The Spark application UI is essential for most Spark users, it is used to monitor the status and resource consumption of the Spark application and get the progress of the application in real-time with many useful metrics (JVM metrics, processed data information, Spark tasks, storage, and shuffle), and the status of executors. When running Spark on Kubernetes, this UI will not be accessible from outside the cluster, and to make it accessible you need to create an ingress for each Spark application service.</p>\n<p>However, the creation of this ingress could be automatized when using the spark-on-k8s-operator.</p>\n<h4>The complexity of submitting and managing the applications</h4>\n<p>The official tool for submitting Spark applications to Kubernetes (and to other cluster managers) is <strong><em>spark-submit</em></strong><em>; </em>it is a bash script in the Spark distribution that processes the provided configurations and runs a Scala application in the <strong>JVM</strong> to submit the Spark application.</p>\n<p>This tool has different problems:</p>\n<ul>\n<li>it needs <strong>JAVA</strong> installed on the\u00a0host</li>\n<li>\n<strong>Spark</strong> must be downloaded to the\u00a0host</li>\n<li>it\u2019s very generic and requires a lot of configuration</li>\n</ul>\n<p>To overcome these issues and limitations, most Spark on Kubernetes users develop their own tools or use third-party tools like spark-on-k8s-operator.</p>\n<h3>spark-on-k8s-operator</h3>\n<figure><img alt=\"Spark on K8s operator architecture\" src=\"https://cdn-images-1.medium.com/max/960/1*KsSZxnJdj32tHAHiEKHZIQ.png\"><figcaption>Spark on K8S operator architecture</figcaption></figure><p><a href=\"https://github.com/GoogleCloudPlatform/spark-on-k8s-operator\"><strong>spark-on-k8s-operator</strong></a> is a Kubernetes operator that can be installed on the cluster to extend the Kubernetes resources list (it adds some CRD like <em>SparkApplication</em>) and processes the created objects to create the spark driver pod based on the provided configurations and monitor the running applications.</p>\n<h4>Features</h4>\n<p>This operator simplifies submitting and managing the Spark jobs for its\u00a0users:</p>\n<ul>\n<li>Enables <strong>declarative</strong> application specification and management of applications through custom resources.</li>\n<li>Provides native <strong>cron</strong> support for running scheduled applications.</li>\n<li>Provides <strong><em>sparkctl</em></strong> which is a dedicated CLI for Spark to use instead of\u00a0<em>kubectl</em>.</li>\n<li>Provides a<strong> mutating admission webhook</strong> to update the driver and executor pods based on certain annotations.</li>\n</ul>\n<h4>Challenges</h4>\n<p>Some users who have a large number of Spark applications and want to reduce duplication in configurations, as well as those who want to configure their applications dynamically, both have issues with the declarative application specification, where it is difficult to create and manage it <strong>programmatically</strong>.</p>\n<p>Additionally, installing and maintaining the operator API is a prerequisite for submitting Spark applications, and this increases the likelihood of <strong>service downtime</strong>, where any failure of the operator API will block the execution of all newly submitted applications. Also, installing this type of operator requires specific <strong>permissions</strong> on the cluster, which might not be possible for some teams, especially in large companies.</p>\n<h3>Spark on K8S python\u00a0package</h3>\n<p><a href=\"https://github.com/hussein-awala/spark-on-k8s\"><strong>spark-on-k8s</strong></a> is a new project that aims to simplify submitting and managing Spark applications with Python. It communicates directly with the Kubernetes API to create the driver pod without the need to install any service on the cluster. It provides a CLI, an optional API, a web server, and integration with\u00a0Airflow.</p>\n<h4>Python client</h4>\n<p>This is a Spark client written in Python used to submit Spark applications to Kubernetes using the <a href=\"https://pypi.org/project/kubernetes/\">Kubernetes Python client</a>. This client uses two other clients internally:</p>\n<ul>\n<li>\n<strong><em>KubernetesClientManager</em></strong>: used to manage Kubernetes credentials and create the client instance.</li>\n<li>\n<strong><em>SparkAppManager</em></strong>: a class contains a list of methods used to manage existing Spark applications.</li>\n</ul>\n<p>Here is an example of submitting a Spark application with this\u00a0client:</p>\n<pre>from spark_on_k8s.client import SparkOnK8S<br><br>client = SparkOnK8S()<br>client.submit_app(<br>    image=\"my-registry/my-image:latest\",<br>    app_path=\"local:///opt/spark/work-dir/my-app.py\",<br>    app_arguments=[\"arg1\", \"arg2\"],<br>    app_name=\"my-app\",<br>    namespace=\"spark-namespace\",<br>    service_account=\"spark-service-account\",<br>    app_waiter=\"log\",<br>    image_pull_policy=\"Never\",<br>    ui_reverse_proxy=True,<br>)</pre>\n<h4>CLI</h4>\n<p>This is a wrapper for the Spark client and the application manager developed in Python using <a href=\"https://click.palletsprojects.com/\">Click</a>. The CLI default values can be configured via environment variables, allowing teams to create a\u00a0<em>.env</em> file to set their default configurations (they also work with the Python\u00a0client).</p>\n<p>Without overriding the default\u00a0values:</p>\n<pre>$ spark-on-k8s app submit --help<br>Usage: spark-on-k8s app submit [OPTIONS] [APP_ARGUMENTS]...<br><br>  Submit a Spark application.<br><br>Options:<br>  --image TEXT                    The docker image to use for the app.  [required]<br>  --path TEXT                     The path to the app to submit.  [required]<br>  -n, --namespace TEXT            The namespace to operate on.  [default: default]<br>...</pre>\n<p>After overriding some of\u00a0them:</p>\n<pre>$ SPARK_ON_K8S_DOCKER_IMAGE=default_docker_image:latest SPARK_ON_K8S_NAMESPACE=spark spark-on-k8s app submit --help<br>Usage: spark-on-k8s app submit [OPTIONS] [APP_ARGUMENTS]...<br><br>  Submit a Spark application.<br><br>Options:<br>  --image TEXT                    The docker image to use for the app.  [default: default_docker_image:latest]<br>  --path TEXT                     The path to the app to submit.  [required]<br>  -n, --namespace TEXT            The namespace to operate on.  [default: spark]<br>...</pre>\n<p>You can find the list of the supported environment variables and their description in the <a href=\"https://github.com/hussein-awala/spark-on-k8s?tab=readme-ov-file#configuration\">project documentation</a>.</p>\n<h4>API and web\u00a0server</h4>\n<p>It\u2019s an optional component that allows listing running applications and their states via a <strong>REST API</strong> or a <strong>web UI</strong>. Both packages use an async version of the application manager developed with the <a href=\"https://pypi.org/project/kubernetes-asyncio/\">kubernetes-asyncio</a> package.</p>\n<p>The web server currently has two\u00a0routes:</p>\n<p><em>&lt;URL&gt;/webserver/apps?namespace=&lt;namespace&gt;</em> to list the applications and their status in a table with the link to the Spark UI for the running applications that have the reverse proxy option activated (<strong>auto-refreshed</strong>):</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Wt8Qig-Q2mfqLw3hhaE9Pw.png\"></figure><p>And<em> &lt;URL&gt;/webserver/ui/&lt;namespace&gt;/&lt;application id&gt;</em> to access the Spark UI through the package <strong>reverse\u00a0proxy</strong>.</p>\n<p>The best way to install the API is by using the developed <a href=\"https://github.com/hussein-awala/spark-on-k8s/tree/main/chart\">helm chart</a>. You can find an example of how to use it in <a href=\"https://github.com/hussein-awala/spark-on-k8s/blob/main/README.md#api-in-production\">the documentation</a>.</p>\n<h4>Airflow</h4>\n<p>For Airflow users, the package provides an operator to submit the Spark applications to Kubernetes with all the Python client features. It works with official Kubernetes connections used by Airflow. The operators can be executed in <strong>deferrable</strong> mode to wait for the job asynchronously.</p>\n<pre>SparkOnK8SOperator(<br>    task_id=\"spark_application\",<br>    kubernetes_conn_id=\"kubernetes_default\",<br>    namespace=\"spark\",<br>    image=\"pyspark-job\",<br>    image_pull_policy=\"Never\",<br>    app_path=\"local:///opt/spark/work-dir/job.py\",<br>    app_arguments=[\"100000\"],<br>    app_name=\"pyspark-job-example\",<br>    service_account=\"spark\",<br>    app_waiter=\"log\",<br>    driver_resources=PodResources(cpu=1, memory=1024, memory_overhead=512),<br>    executor_resources=PodResources(cpu=1, memory=1024, memory_overhead=512),<br>    executor_instances=ExecutorInstances(min=0, max=5, initial=5),<br>    ui_reverse_proxy=True,<br>    deferrable=True,<br>)</pre>\n<h4>Install the\u00a0package</h4>\n<p>The package is available on PyPI, you can run this command to install\u00a0it:</p>\n<pre># Install the core pacakge<br>pip install \"spark-on-k8s\"<br><br># Install it with the API extra<br>pip install \"spark-on-k8s[api]\"<br><br># Install it with Airflow extra<br>pip install \"spark-on-k8s[airflow]\"<br><br># Install it with all the extras<br>pip install \"spark-on-k8s[api,airflow]\"</pre>\n<p>You can also use the docker image that contains the core package and the API\u00a0extra:</p>\n<pre># pull from ghcr.io<br>docker pull ghcr.io/hussein-awala/spark-on-k8s:&lt;version&gt;<br><br># or from docker hub<br>docker pull husseinawala/spark-on-k8s:&lt;version&gt;</pre>\n<h4>What\u2019s next</h4>\n<p>The package is currently in <strong>active development</strong>, and many new features and improvements will be added in future releases, such as support for creating ephemeral secrets to store and use application secrets, integration with cloud providers' services (S3, GCS, Glue,\u00a0\u2026), and new features in the API and web server to make them an alternative solution to the CLI. You can check the projects\u2019 <a href=\"https://github.com/hussein-awala/spark-on-k8s/blob/main/TODO.md\">TODO list</a> for more\u00a0details.</p>\n<h3>Summary</h3>\n<p>Although running Spark on Kubernetes makes it more powerful, cheaper, and more flexible, managing Spark applications on Kubernetes is a significant challenge for many users. Users rely on some third-party tools to simplify the management of their applications, such as spark-on-k8s-operator, but these tools do not solve all their problems and in some cases introduce new challenges. This blog introduces a new Python package that can help overcome some of these issues and provides new features to simplify the management of Spark applications, with a promising roadmap.</p>\n<h3>Stackademic \ud83c\udf93</h3>\n<p>Thank you for reading until the end. Before you\u00a0go:</p>\n<ul>\n<li>Please consider <strong>clapping</strong> and <strong>following</strong> the writer!\u00a0\ud83d\udc4f</li>\n<li>Follow us <a href=\"https://twitter.com/stackademichq\"><strong>X</strong></a> | <a href=\"https://www.linkedin.com/company/stackademic\"><strong>LinkedIn</strong></a> | <a href=\"https://www.youtube.com/c/stackademic\"><strong>YouTube</strong></a> |\u00a0<a href=\"https://discord.gg/in-plain-english-709094664682340443\"><strong>Discord</strong></a>\n</li>\n<li>Visit our other platforms: <a href=\"https://plainenglish.io/\"><strong>In Plain English</strong></a> | <a href=\"https://cofeed.app/\"><strong>CoFeed</strong></a> |\u00a0<a href=\"https://differ.blog/\"><strong>Differ</strong></a>\n</li>\n<li>More content at <a href=\"https://stackademic.com/\"><strong>Stackademic.com</strong></a>\n</li>\n</ul>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=695b79914b59\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://blog.stackademic.com/the-golden-path-for-spark-on-kubernetes-695b79914b59\">The Golden Path for Spark On Kubernetes</a> was originally published in <a href=\"https://blog.stackademic.com/\">Stackademic</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","enclosure":{},"categories":["python","kubernetes","spark","airflow"]},{"title":"Airflow Setup and Teardown Tasks","pubDate":"2023-08-21 08:20:48","link":"https://medium.com/apache-airflow/airflow-setup-and-teardown-tasks-e4ba2a4fefb6?source=rss-8725a0967242------2","guid":"https://medium.com/p/e4ba2a4fefb6","author":"Hussein Awala","thumbnail":"","description":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*UYPUW9U4IDNMRA7t\"><figcaption>Photo by <a href=\"https://unsplash.com/@towfiqu999999?utm_source=medium&amp;utm_medium=referral\">Towfiqu barbhuiya</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><p>In the realm of big data, most data workflows require the creation and configuration of certain infrastructure resources before they commence. Additionally, these resources need to be reconfigured and cleaned up upon the termination of their execution. This necessity has consistently posed a challenge for data engineers, as the schedulers commonly used do not offer an easy way to implement these\u00a0tasks.</p>\n<h4>A Real-World Example</h4>\n<p>Consider the scenario of Spark ETLs (Extract, Transform, Load pipelines) running on Google Cloud Platform\u2019s DataProc service. GCP suggests the creation of job-scoped ephemeral clusters for executing these tasks by initiating a specific job and shutting down upon the job\u2019s completion.</p>\n<p>The DataProc service is optimized to create clusters of this type in approximately 90 seconds. This speed signifies that opting for a dedicated cluster per job proves to be cost-efficient, exceptionally scalable, and boasts minimal latency. Furthermore, users have the flexibility to generate and utilize a cluster for a collection of jobs that share similar characteristics, thus mitigating execution latency effectively.</p>\n<h3>Airflow Before AIP-52 Implementation</h3>\n<p>Airflow stands out as the most extensively employed scheduler within the data realm. Most GCP users opt for Airflow to orchestrate their workflows, primarily due to GCP\u2019s provision of a fully managed workflow orchestration service known as Cloud Composer, built on Apache\u00a0Airflow.</p>\n<p>Despite Airflow\u2019s support for different types of callback functions at DAG and task levels, creating workflows akin to ephemeral DataProc clusters has historically presented a challenge. In Airflow, these callbacks lack a transparent state, a built-in retry mechanism and are not conducive to easy result utilization or interaction with other tasks. Users were faced with diverse alternatives for implementing such workflows.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*cCDrCBwMsHdpBO-D\"><figcaption>Photo by <a href=\"https://unsplash.com/@amyames?utm_source=medium&amp;utm_medium=referral\">Amy Elting</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><h4>Utilizing Standard\u00a0Tasks:</h4>\n<p>The initial approach involved the creation and subsequent deletion of the cluster using regular\u00a0tasks:</p>\n<pre># a cluster per dag<br>create_cluster &gt;&gt; run_job1 &gt;&gt; run_job2 &gt;&gt; run_job3 &gt;&gt; delete_cluster<br><br># a cluster per jobs group<br>(<br>  create_cluster1 &gt;&gt; run_job1 &gt;&gt; delete_cluster_1<br>) &gt;&gt; (<br>  create_cluster2 &gt;&gt; run_job2 &gt;&gt; run_job3 &gt;&gt; delete_cluster_2<br>)</pre>\n<p>This solution appears ideal in scenarios where an assumption is made that jobs will invariably conclude in a successful state, a technically implausible presumption. However, when accommodating failure scenarios, the solution becomes overly intricate.</p>\n<p>To address failures, we had to set up the delete_cluster task as a downstream task for each job task, to run it after all jobs, regardless of their state. This is feasible in cases where the DAG\u2019s dependency structure isn\u2019t complex, allowing for the inclusion of all job tasks as upstream dependencies for the delete_cluster task. Subsequently, the trigger rule for the delete_cluster task is adjusted to ALL_DONE.</p>\n<p>A secondary challenge emerges concerning the re-execution of the create_cluster task following the resolution of an issue, mainly when the job task state is cleared. The resolution entailed manual re-execution of the task either through the user interface (UI) or the command-line interface (CLI), followed by waiting for it before clearing the job task state along with its downstream tasks to resume execution.</p>\n<h4>Utilizing DAG/Task Callbacks:</h4>\n<p>The alternative approach involved leveraging the on_execute_callback to create the cluster and employing on_success_callback and on_failure_callback to handle cluster deletion:</p>\n<pre>def create_cluster(context):<br>  # create DataProc cluster<br><br>def delete_cluster(context):<br>  # delete DataProc cluster<br><br># a cluster per dag<br>DAG(<br>  ...,<br>  on_execute_callback=create_cluster,<br>  on_success_callback=delete_cluster,<br>  on_failure_callback=delete_cluster,<br>):<br>  run_job1 &gt;&gt; run_job2 &gt;&gt; run_job3<br><br># a cluster per jobs group<br># a cluster for job1 and a cluster for job 2 and 3<br>def create_cluster_if_not_exist(context):<br>  # check if the DataProc cluster exists and create it if not<br><br>DAG(<br>  ...,<br>):<br>  run_job1 = RunJob(<br>    ...,<br>    on_execute_callback=create_cluster,<br>    on_success_callback=delete_cluster,<br>    on_failure_callback=delete_cluster,<br>  )<br><br>  run_job2 = RunJob(<br>    ...,<br>    on_execute_callback=create_cluster,<br>    on_failure_callback=delete_cluster,<br>  )<br><br>  run_job3 = RunJob(<br>    ...,<br>    on_execute_callback=create_cluster_if_not_exist,<br>    on_success_callback=delete_cluster,<br>    on_failure_callback=delete_cluster,<br>  )<br><br>  run_job1 &gt;&gt; run_job2 &gt;&gt; run_job3</pre>\n<p>In simpler scenarios, this approach might suffice to fulfill the requirements. However, outlining the logic graph for the callback methods can prove challenging.</p>\n<p>This solution comes with certain limitations. Failures in the callback methods don\u2019t halt task execution; they\u2019re considered \u201csoft fails.\u201d Consequently, ensuring cluster availability before execution isn\u2019t guaranteed.</p>\n<p>Additionally, the on_execute_callback and on_success_callback are regarded as integral to the task, potentially affecting task monitoring due to the inclusion of their execution times in the job\u2019s execution duration.</p>\n<h4>Incorporating Cluster Management Code within the Job\u00a0Task:</h4>\n<p>In this approach, we directly integrate the creation and deletion of clusters into the job operator\u2019s execute method. By doing so, if the method encounters a failure, the task itself fails. However, caution must be exercised during cluster deletion to prevent triggering a complete task retry in case of\u00a0failure:</p>\n<pre>class DataProcJob(BaseOperator):<br>    def execute(context):<br>      try:<br>        create_cluster()<br>        run_job()<br>      finally:<br>        # to avoid failing the task when delete cluster fails<br>        # we need to run it in a loop with try except pass statement<br>        for i in range(max_delete_attemps):<br>          try:<br>            delete_cluster_if_exists()<br>            break<br>          except:<br>            pass<br><br>    def on_kill():<br>      # if the task is forcelly killed, we need to delete the cluster<br>      for i in range(max_delete_attemps):<br>        try:<br>          delete_cluster_if_exists()<br>          break<br>        except:<br>          pass      </pre>\n<p>This solution makes the logic within the operators more intricate, and it still faces the same challenge regarding task monitoring as the second\u00a0option.</p>\n<h3>Airflow 2.7.0 and\u00a0AIP-52</h3>\n<p>An exciting advancement was introduced in Airflow 2.7.0 through <a href=\"https://cwiki.apache.org/confluence/x/GIvFDg\">AIP-52</a> (Airflow Improvement Proposals). This enhancement centers around the support for automatic setup and teardown\u00a0tasks.</p>\n<p>This new feature enables the definition of setup and teardown tasks for a set of tasks. Airflow takes charge of executing the setup task before initiating these tasks and executes the teardown task after all tasks have terminated (irrespective of their individual states). Moreover, when a task\u2019s state is cleared, Airflow also clears the states of its associated setup and teardown tasks. This ensures that the task is not executed before its prerequisites are arranged, and it\u2019s not terminated without the necessary clean-up of the environment.</p>\n<p>Example:</p>\n<a href=\"https://medium.com/media/79982d898ae921bc7329142d611149fe/href\">https://medium.com/media/79982d898ae921bc7329142d611149fe/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/936/1*pAnzidk58iuq3YeoMXmgMA.png\"><figcaption>DAG graph</figcaption></figure><p>In the given example, Airflow executes the setup_task before executing the remaining tasks, given that it serves as an upstream task for all others. Upon the completion of tasks t1, t2, and t3 with various end states (such as success, failure, skipping, or encountering upstream failure), Airflow proceeds to execute the teardown_task. Additionally, if any of the state of the mentioned tasks (t1, t2, or t3 ) is cleared, Airflow will clear the state of their associated setup and teardown tasks. It will then execute the setup task before the cleared task, even if it isn\u2019t a direct upstream task (note: there is a <a href=\"https://github.com/apache/airflow/issues/33561\">known issue</a> in this scenario, which will be resolved in version\u00a02.7.1).</p>\n<h4>DataProc DAG\u00a0code:</h4>\n<p>In the following code, I implement a method to create a setup and a teardown task for each DataprocSubmitJobOperator task, and I use this method to configure three\u00a0jobs:</p>\n<a href=\"https://medium.com/media/f208bfb4c8c7aabde9f345a10482c3ac/href\">https://medium.com/media/f208bfb4c8c7aabde9f345a10482c3ac/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*dSKpRYOnn3kZayqsi_u9bw.png\"><figcaption>DataProc DAG with setup/teardown tasks</figcaption></figure><p>The introduced feature also extends support to dynamic tasks and dynamic task groups. To illustrate, we can restructure the initial DAG by substituting the method previously employed for task creation with a task group. Subsequently, we can initialize a task group and utilize the expand_kwargs method to dynamically generate clusters and submit\u00a0jobs.</p>\n<a href=\"https://medium.com/media/6d94d0b9979b8a1b8d8ec39cded50dfe/href\">https://medium.com/media/6d94d0b9979b8a1b8d8ec39cded50dfe/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*sGwostwM9fXyj0yQ-TXLxw.png\"><figcaption>DataProc DAG with setup/teardown tasks and dynamic task\u00a0group</figcaption></figure><h4>Contrasting with Previous Solutions:</h4>\n<p>Comparing the approach involving setup and teardown tasks with the initial three methods, we notice resemblances to the first approach we discussed. Here, the setup and teardown tasks serve as Airflow tasks, but a notable distinction lies in eliminating the requirement to create branching structures for the delete cluster task across all job tasks. Additionally, there\u2019s no need to manually adjust its state to ALL_DONE. Moreover, the create cluster task is triggered whenever an issue is resolved, and the job task is\u00a0reset.</p>\n<p>On the other hand, when compared to the second and third solutions, any failure within the setup task functions as an impediment to the job tasks. In the monitoring context, the setup and job tasks operate independently, facilitating the individual customization of retries, timeouts, and notifications for each. Regarding the teardown task, the choice of whether a DagRun should be marked as a failure upon encountering issues in the teardown task can be dictated by the on_failure_fail_dagrun parameter.</p>\n<h4>Other use\u00a0cases:</h4>\n<ul>\n<li>\n<strong>DynamoDB Scaling:</strong> Before a batch ingestion task, increase the DynamoDB write capacity to accommodate the incoming data load. After the task concludes, revert the capacity to its original\u00a0state.</li>\n<li>\n<strong>Ephemeral Cluster Management: </strong>Analogous to the GCP DataProc case, it\u2019s common to create temporary AWS EMR and Databricks clusters before running spark batch jobs. Once the desired results are obtained, these clusters are promptly\u00a0removed.</li>\n<li>\n<strong>ML Model Training Infrastructure:</strong> Before initiating the machine learning model training, provision virtual machines equipped with GPU instances. Following the training job\u2019s completion, scale down the resources to optimize utilization.</li>\n</ul>\n<h3>Summary:</h3>\n<p>In the world of big data workflows, managing pipeline jobs infrastructure has been a challenge. In Airflow, initial solutions involved creating and deleting infrastructure resources through regular tasks or callbacks, which had drawbacks. Airflow 2.7.0 and AIP-52 changed the game. Automatic setup and teardown tasks were introduced, streamlining infrastructure management and task execution. This combines the benefits of the old solutions without complexities. The setup and teardown tasks adapt to issues and are flexible for retries, timeout, and notifications.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=e4ba2a4fefb6\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/apache-airflow/airflow-setup-and-teardown-tasks-e4ba2a4fefb6\">Airflow Setup and Teardown Tasks</a> was originally published in <a href=\"https://medium.com/apache-airflow\">Apache Airflow</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","content":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*UYPUW9U4IDNMRA7t\"><figcaption>Photo by <a href=\"https://unsplash.com/@towfiqu999999?utm_source=medium&amp;utm_medium=referral\">Towfiqu barbhuiya</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><p>In the realm of big data, most data workflows require the creation and configuration of certain infrastructure resources before they commence. Additionally, these resources need to be reconfigured and cleaned up upon the termination of their execution. This necessity has consistently posed a challenge for data engineers, as the schedulers commonly used do not offer an easy way to implement these\u00a0tasks.</p>\n<h4>A Real-World Example</h4>\n<p>Consider the scenario of Spark ETLs (Extract, Transform, Load pipelines) running on Google Cloud Platform\u2019s DataProc service. GCP suggests the creation of job-scoped ephemeral clusters for executing these tasks by initiating a specific job and shutting down upon the job\u2019s completion.</p>\n<p>The DataProc service is optimized to create clusters of this type in approximately 90 seconds. This speed signifies that opting for a dedicated cluster per job proves to be cost-efficient, exceptionally scalable, and boasts minimal latency. Furthermore, users have the flexibility to generate and utilize a cluster for a collection of jobs that share similar characteristics, thus mitigating execution latency effectively.</p>\n<h3>Airflow Before AIP-52 Implementation</h3>\n<p>Airflow stands out as the most extensively employed scheduler within the data realm. Most GCP users opt for Airflow to orchestrate their workflows, primarily due to GCP\u2019s provision of a fully managed workflow orchestration service known as Cloud Composer, built on Apache\u00a0Airflow.</p>\n<p>Despite Airflow\u2019s support for different types of callback functions at DAG and task levels, creating workflows akin to ephemeral DataProc clusters has historically presented a challenge. In Airflow, these callbacks lack a transparent state, a built-in retry mechanism and are not conducive to easy result utilization or interaction with other tasks. Users were faced with diverse alternatives for implementing such workflows.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*cCDrCBwMsHdpBO-D\"><figcaption>Photo by <a href=\"https://unsplash.com/@amyames?utm_source=medium&amp;utm_medium=referral\">Amy Elting</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><h4>Utilizing Standard\u00a0Tasks:</h4>\n<p>The initial approach involved the creation and subsequent deletion of the cluster using regular\u00a0tasks:</p>\n<pre># a cluster per dag<br>create_cluster &gt;&gt; run_job1 &gt;&gt; run_job2 &gt;&gt; run_job3 &gt;&gt; delete_cluster<br><br># a cluster per jobs group<br>(<br>  create_cluster1 &gt;&gt; run_job1 &gt;&gt; delete_cluster_1<br>) &gt;&gt; (<br>  create_cluster2 &gt;&gt; run_job2 &gt;&gt; run_job3 &gt;&gt; delete_cluster_2<br>)</pre>\n<p>This solution appears ideal in scenarios where an assumption is made that jobs will invariably conclude in a successful state, a technically implausible presumption. However, when accommodating failure scenarios, the solution becomes overly intricate.</p>\n<p>To address failures, we had to set up the delete_cluster task as a downstream task for each job task, to run it after all jobs, regardless of their state. This is feasible in cases where the DAG\u2019s dependency structure isn\u2019t complex, allowing for the inclusion of all job tasks as upstream dependencies for the delete_cluster task. Subsequently, the trigger rule for the delete_cluster task is adjusted to ALL_DONE.</p>\n<p>A secondary challenge emerges concerning the re-execution of the create_cluster task following the resolution of an issue, mainly when the job task state is cleared. The resolution entailed manual re-execution of the task either through the user interface (UI) or the command-line interface (CLI), followed by waiting for it before clearing the job task state along with its downstream tasks to resume execution.</p>\n<h4>Utilizing DAG/Task Callbacks:</h4>\n<p>The alternative approach involved leveraging the on_execute_callback to create the cluster and employing on_success_callback and on_failure_callback to handle cluster deletion:</p>\n<pre>def create_cluster(context):<br>  # create DataProc cluster<br><br>def delete_cluster(context):<br>  # delete DataProc cluster<br><br># a cluster per dag<br>DAG(<br>  ...,<br>  on_execute_callback=create_cluster,<br>  on_success_callback=delete_cluster,<br>  on_failure_callback=delete_cluster,<br>):<br>  run_job1 &gt;&gt; run_job2 &gt;&gt; run_job3<br><br># a cluster per jobs group<br># a cluster for job1 and a cluster for job 2 and 3<br>def create_cluster_if_not_exist(context):<br>  # check if the DataProc cluster exists and create it if not<br><br>DAG(<br>  ...,<br>):<br>  run_job1 = RunJob(<br>    ...,<br>    on_execute_callback=create_cluster,<br>    on_success_callback=delete_cluster,<br>    on_failure_callback=delete_cluster,<br>  )<br><br>  run_job2 = RunJob(<br>    ...,<br>    on_execute_callback=create_cluster,<br>    on_failure_callback=delete_cluster,<br>  )<br><br>  run_job3 = RunJob(<br>    ...,<br>    on_execute_callback=create_cluster_if_not_exist,<br>    on_success_callback=delete_cluster,<br>    on_failure_callback=delete_cluster,<br>  )<br><br>  run_job1 &gt;&gt; run_job2 &gt;&gt; run_job3</pre>\n<p>In simpler scenarios, this approach might suffice to fulfill the requirements. However, outlining the logic graph for the callback methods can prove challenging.</p>\n<p>This solution comes with certain limitations. Failures in the callback methods don\u2019t halt task execution; they\u2019re considered \u201csoft fails.\u201d Consequently, ensuring cluster availability before execution isn\u2019t guaranteed.</p>\n<p>Additionally, the on_execute_callback and on_success_callback are regarded as integral to the task, potentially affecting task monitoring due to the inclusion of their execution times in the job\u2019s execution duration.</p>\n<h4>Incorporating Cluster Management Code within the Job\u00a0Task:</h4>\n<p>In this approach, we directly integrate the creation and deletion of clusters into the job operator\u2019s execute method. By doing so, if the method encounters a failure, the task itself fails. However, caution must be exercised during cluster deletion to prevent triggering a complete task retry in case of\u00a0failure:</p>\n<pre>class DataProcJob(BaseOperator):<br>    def execute(context):<br>      try:<br>        create_cluster()<br>        run_job()<br>      finally:<br>        # to avoid failing the task when delete cluster fails<br>        # we need to run it in a loop with try except pass statement<br>        for i in range(max_delete_attemps):<br>          try:<br>            delete_cluster_if_exists()<br>            break<br>          except:<br>            pass<br><br>    def on_kill():<br>      # if the task is forcelly killed, we need to delete the cluster<br>      for i in range(max_delete_attemps):<br>        try:<br>          delete_cluster_if_exists()<br>          break<br>        except:<br>          pass      </pre>\n<p>This solution makes the logic within the operators more intricate, and it still faces the same challenge regarding task monitoring as the second\u00a0option.</p>\n<h3>Airflow 2.7.0 and\u00a0AIP-52</h3>\n<p>An exciting advancement was introduced in Airflow 2.7.0 through <a href=\"https://cwiki.apache.org/confluence/x/GIvFDg\">AIP-52</a> (Airflow Improvement Proposals). This enhancement centers around the support for automatic setup and teardown\u00a0tasks.</p>\n<p>This new feature enables the definition of setup and teardown tasks for a set of tasks. Airflow takes charge of executing the setup task before initiating these tasks and executes the teardown task after all tasks have terminated (irrespective of their individual states). Moreover, when a task\u2019s state is cleared, Airflow also clears the states of its associated setup and teardown tasks. This ensures that the task is not executed before its prerequisites are arranged, and it\u2019s not terminated without the necessary clean-up of the environment.</p>\n<p>Example:</p>\n<a href=\"https://medium.com/media/79982d898ae921bc7329142d611149fe/href\">https://medium.com/media/79982d898ae921bc7329142d611149fe/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/936/1*pAnzidk58iuq3YeoMXmgMA.png\"><figcaption>DAG graph</figcaption></figure><p>In the given example, Airflow executes the setup_task before executing the remaining tasks, given that it serves as an upstream task for all others. Upon the completion of tasks t1, t2, and t3 with various end states (such as success, failure, skipping, or encountering upstream failure), Airflow proceeds to execute the teardown_task. Additionally, if any of the state of the mentioned tasks (t1, t2, or t3 ) is cleared, Airflow will clear the state of their associated setup and teardown tasks. It will then execute the setup task before the cleared task, even if it isn\u2019t a direct upstream task (note: there is a <a href=\"https://github.com/apache/airflow/issues/33561\">known issue</a> in this scenario, which will be resolved in version\u00a02.7.1).</p>\n<h4>DataProc DAG\u00a0code:</h4>\n<p>In the following code, I implement a method to create a setup and a teardown task for each DataprocSubmitJobOperator task, and I use this method to configure three\u00a0jobs:</p>\n<a href=\"https://medium.com/media/f208bfb4c8c7aabde9f345a10482c3ac/href\">https://medium.com/media/f208bfb4c8c7aabde9f345a10482c3ac/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*dSKpRYOnn3kZayqsi_u9bw.png\"><figcaption>DataProc DAG with setup/teardown tasks</figcaption></figure><p>The introduced feature also extends support to dynamic tasks and dynamic task groups. To illustrate, we can restructure the initial DAG by substituting the method previously employed for task creation with a task group. Subsequently, we can initialize a task group and utilize the expand_kwargs method to dynamically generate clusters and submit\u00a0jobs.</p>\n<a href=\"https://medium.com/media/6d94d0b9979b8a1b8d8ec39cded50dfe/href\">https://medium.com/media/6d94d0b9979b8a1b8d8ec39cded50dfe/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*sGwostwM9fXyj0yQ-TXLxw.png\"><figcaption>DataProc DAG with setup/teardown tasks and dynamic task\u00a0group</figcaption></figure><h4>Contrasting with Previous Solutions:</h4>\n<p>Comparing the approach involving setup and teardown tasks with the initial three methods, we notice resemblances to the first approach we discussed. Here, the setup and teardown tasks serve as Airflow tasks, but a notable distinction lies in eliminating the requirement to create branching structures for the delete cluster task across all job tasks. Additionally, there\u2019s no need to manually adjust its state to ALL_DONE. Moreover, the create cluster task is triggered whenever an issue is resolved, and the job task is\u00a0reset.</p>\n<p>On the other hand, when compared to the second and third solutions, any failure within the setup task functions as an impediment to the job tasks. In the monitoring context, the setup and job tasks operate independently, facilitating the individual customization of retries, timeouts, and notifications for each. Regarding the teardown task, the choice of whether a DagRun should be marked as a failure upon encountering issues in the teardown task can be dictated by the on_failure_fail_dagrun parameter.</p>\n<h4>Other use\u00a0cases:</h4>\n<ul>\n<li>\n<strong>DynamoDB Scaling:</strong> Before a batch ingestion task, increase the DynamoDB write capacity to accommodate the incoming data load. After the task concludes, revert the capacity to its original\u00a0state.</li>\n<li>\n<strong>Ephemeral Cluster Management: </strong>Analogous to the GCP DataProc case, it\u2019s common to create temporary AWS EMR and Databricks clusters before running spark batch jobs. Once the desired results are obtained, these clusters are promptly\u00a0removed.</li>\n<li>\n<strong>ML Model Training Infrastructure:</strong> Before initiating the machine learning model training, provision virtual machines equipped with GPU instances. Following the training job\u2019s completion, scale down the resources to optimize utilization.</li>\n</ul>\n<h3>Summary:</h3>\n<p>In the world of big data workflows, managing pipeline jobs infrastructure has been a challenge. In Airflow, initial solutions involved creating and deleting infrastructure resources through regular tasks or callbacks, which had drawbacks. Airflow 2.7.0 and AIP-52 changed the game. Automatic setup and teardown tasks were introduced, streamlining infrastructure management and task execution. This combines the benefits of the old solutions without complexities. The setup and teardown tasks adapt to issues and are flexible for retries, timeout, and notifications.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=e4ba2a4fefb6\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/apache-airflow/airflow-setup-and-teardown-tasks-e4ba2a4fefb6\">Airflow Setup and Teardown Tasks</a> was originally published in <a href=\"https://medium.com/apache-airflow\">Apache Airflow</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","enclosure":{},"categories":["gcp","big-data","orchestration","etl","airflow"]},{"title":"Are You Using Parquet with Pandas in the Right Way?","pubDate":"2023-07-24 07:49:05","link":"https://medium.com/munchy-bytes/are-you-using-parquet-with-pandas-in-the-right-way-595c9ee7112?source=rss-8725a0967242------2","guid":"https://medium.com/p/595c9ee7112","author":"Hussein Awala","thumbnail":"","description":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*TAtJo4iw1SYbj-pt\"><figcaption>Photo by <a href=\"https://unsplash.com/@markfb?utm_source=medium&amp;utm_medium=referral\">Mark Fletcher-Brown</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><p>Apache Parquet is an open-source columnar storage format that is designed to efficiently store and process large amounts of structured data. It was developed as part of the Apache Hadoop ecosystem and is widely used in big data processing frameworks like Apache\u00a0Spark.</p>\n<h4>Why do we use\u00a0Parquet?</h4>\n<p>The Parquet format is specifically optimized for analytical workloads and is commonly used in data engineering and data science tasks. It provides several key benefits that make it an excellent choice for storing and processing data:</p>\n<ol>\n<li>\n<strong>Columnar Storage</strong>: Unlike row-based storage formats, Parquet organizes data in a columnar fashion, which means that all values of a single column are stored together. This layout allows for better compression and encoding of similar data, leading to reduced storage space and improved query performance.</li>\n<li>\n<strong>Compression</strong>: Parquet supports various compression algorithms, such as Snappy, Gzip, and Zstandard, which further contribute to reducing storage requirements and can speed up data reading from\u00a0disk.</li>\n<li>\n<strong>Schema Evolution</strong>: Parquet allows for schema evolution, meaning you can easily add, remove, or modify columns in the data without requiring a full rewrite of the dataset. This flexibility is particularly useful in scenarios where the schema evolves over\u00a0time.</li>\n<li>\n<strong>Cross-Platform Compatibility</strong>: Parquet is designed to be a portable format, enabling seamless data exchange between different systems and programming languages.</li>\n</ol>\n<h4>Why do we use Parquet with\u00a0pandas?</h4>\n<p>Pandas is a popular Python library for data manipulation and analysis. When working with large datasets in pandas, the performance of traditional file formats like CSV or JSON can be suboptimal due to their inherent limitations, such as row-based storage and lack of compression.</p>\n<p>By using Parquet files with pandas, you can take advantage of the benefits provided by the columnar storage format. Reading data from Parquet files into pandas DataFrames can be significantly faster compared to row-based formats, especially when dealing with large datasets. Additionally, since pandas natively supports Parquet, it can efficiently leverage the format\u2019s compression and encoding techniques, resulting in reduced memory usage and faster data processing.</p>\n<p>Moreover, Parquet\u2019s schema evolution feature aligns well with pandas\u2019 flexibility, allowing you to easily handle data with varying column structures or add new columns to existing DataFrames without much overhead.</p>\n<h3>Parquet file structure</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/601/1*L9O-Lj4V-QcsMAUpvs4tcA.gif\"><figcaption>Apache Parquet file structure</figcaption></figure><p>At its core, a Parquet file is composed of a collection of <strong>row groups</strong>, and each row group contains a set of <strong>column chunks</strong>. These column chunks store the actual data for each column of the\u00a0dataset.</p>\n<h4>Magic Number:</h4>\n<p>The magic number is a sequence of bytes at the beginning of a Parquet file (in the header), that identifies it as a Parquet file format. It is a fixed set of bytes (PAR1) that all Parquet files must begin with, allowing software to quickly identify whether a file is in the Parquet\u00a0format.</p>\n<h4>Row Groups</h4>\n<p>A Parquet file is divided into one or more row groups. Each row group is essentially a horizontal partition of the dataset, containing a specific number of rows. This design enables efficient read and write operations, as it allows applications to read only the required row groups instead of scanning the entire\u00a0file.</p>\n<h4>Pages</h4>\n<p>Within each row group, data is further divided into pages. Pages are the smallest unit of read and write in Parquet files. They represent a contiguous set of values for a particular column. Pages can be compressed independently, allowing for efficient compression and decompression.</p>\n<h4>Column Statistics</h4>\n<p>For every column chunk within a row group, Parquet can store column statistics. These statistics provide summary information about the data in each column, such as minimum and maximum values, the number of nulls, and other relevant data characteristics. Storing column statistics is beneficial for query optimization, as it allows Parquet readers to skip unnecessary row groups during data scanning, thus improving query performance.</p>\n<h4>Footer</h4>\n<p>The footer is located at the end of the Parquet file and contains essential metadata about the file\u2019s structure. It includes information about the schema of the dataset, the compression algorithms used for each column chunk, and the data location of each row group. The footer provides a high-level overview of the file, enabling quick access to critical metadata for processing the Parquet\u00a0file.</p>\n<h3>Processing Parquet files using\u00a0pandas</h3>\n<p>When working with Parquet files in pandas, you have the flexibility to choose between two engines: <em>fastparquet</em> and <em>pyarrow</em>. Both engines are third-party libraries that provide support for reading and writing Parquet files, and pandas seamlessly integrates with them to offer enhanced performance and efficiency.</p>\n<p><em>Fastparquet</em> is a popular Python library optimized for fast reading and writing of Parquet files. It is known for its speed and low memory footprint, making it an excellent choice for working with large datasets.</p>\n<p><em>Pyarrow</em> is part of the Apache Arrow project and is designed to provide efficient data interchange between different systems and languages. It offers seamless integration with pandas, allowing for fast and optimized data processing with Parquet\u00a0files.</p>\n<p>In this section, I will explain how to predicate your filters to <em>pyarrow</em> to reduce the size of the dataset processed by pandas, in order to reduce the processing time and the resource consumption.</p>\n<h4>Creating a simple file for\u00a0testing</h4>\n<pre>import pandas as pd<br>import numpy as np<br><br>parquet_file_path = \"test_data.parquet\"<br><br># Number of rows to generate<br>num_rows = 10**8 # 100M<br><br># Generate the DataFrame<br>data = {<br>    \"user_id\": np.arange(num_rows),<br>    \"value\": np.random.randint(-10000, 10001, size=num_rows)<br>}<br>df = pd.DataFrame(data)<br><br># Write the result to a Parquet file with 20 row groups (5M records per row group)<br>df.to_parquet(parquet_file_path, index=False, row_group_size=5 * 10**6)</pre>\n<p>The above script creates a single Parquet file sorted by <em>user_id</em>, consists of 20 row groups, each row group contains 5 millions rows. You can check the row groups metadata using pyarrow parquet\u00a0module:</p>\n<pre>import pyarrow.parquet as pq<br><br>parquet_file = pq.ParquetFile(parquet_file_path)<br><br>for i in range(parquet_file.metadata.num_row_groups):<br>    user_id_col_stats = parquet_file.metadata.row_group(i).column(0).statistics<br>    print(f\"row group: {i}, num of rows: {user_id_col_stats.num_values}, min: {user_id_col_stats.min}, max: {user_id_col_stats.max}\")</pre>\n<p>And here is the\u00a0output:</p>\n<pre>row group: 0, num of rows: 5000000, min: 0, max: 4999999<br>row group: 1, num of rows: 5000000, min: 5000000, max: 9999999<br>row group: 2, num of rows: 5000000, min: 10000000, max: 14999999<br>row group: 3, num of rows: 5000000, min: 15000000, max: 19999999<br>row group: 4, num of rows: 5000000, min: 20000000, max: 24999999<br>row group: 5, num of rows: 5000000, min: 25000000, max: 29999999<br>row group: 6, num of rows: 5000000, min: 30000000, max: 34999999<br>row group: 7, num of rows: 5000000, min: 35000000, max: 39999999<br>row group: 8, num of rows: 5000000, min: 40000000, max: 44999999<br>row group: 9, num of rows: 5000000, min: 45000000, max: 49999999<br>row group: 10, num of rows: 5000000, min: 50000000, max: 54999999<br>row group: 11, num of rows: 5000000, min: 55000000, max: 59999999<br>row group: 12, num of rows: 5000000, min: 60000000, max: 64999999<br>row group: 13, num of rows: 5000000, min: 65000000, max: 69999999<br>row group: 14, num of rows: 5000000, min: 70000000, max: 74999999<br>row group: 15, num of rows: 5000000, min: 75000000, max: 79999999<br>row group: 16, num of rows: 5000000, min: 80000000, max: 84999999<br>row group: 17, num of rows: 5000000, min: 85000000, max: 89999999<br>row group: 18, num of rows: 5000000, min: 90000000, max: 94999999<br>row group: 19, num of rows: 5000000, min: 95000000, max: 99999999</pre>\n<h4>Querying the file using\u00a0pandas</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*w7iGKXO549p3YHVV\"><figcaption>Photo by <a href=\"https://unsplash.com/@neonbrand?utm_source=medium&amp;utm_medium=referral\">Kenny Eliason</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><p>Typically, when working with Parquet files in pandas, the common approach involves loading the data into a pandas DataFrame and then performing the necessary data processing tasks on the loaded DataFrame:</p>\n<pre>%%time<br><br>pd.read_parquet(parquet_file_path).query(\"user_id == 8767068\")<br><br>CPU times: user 4.07 s, sys: 12.5 s, total: 16.6 s<br>Wall time: 31.6 s<br><br>&gt; {'user_id': {8767068: 8767068}, 'value': {8767068: 5893}}</pre>\n<p>As observed, the process of scanning and loading the entire Parquet file into a pandas DataFrame, followed by filtering to obtain matched rows, took approximately 16 seconds. However, pandas does not inherently pushdown filters to the Parquet engine, and it does not fully utilize the benefits of Parquet metadata.</p>\n<p>To optimize the process, we can manually pushdown our filters to the <em>pyarrow</em> engine, by adding a predicate filter to the<em> </em>reading<em> </em>method, leveraging the file metadata and <em>pyarrow</em>\u2019s processing optimizations. By doing so, we can significantly improve the query performance and reduce the processing time for data filtering and retrieval:</p>\n<pre>%%time<br><br>pd.read_parquet(parquet_file_path, filters=[(\"user_id\", \"=\", 8767068)]).to_dict()<br><br>CPU times: user 132 ms, sys: 120 ms, total: 252 ms<br>Wall time: 868 ms<br><br>&gt; {'user_id': {0: 8767068}, 'value': {0: 5893}}</pre>\n<p>In the last query, locating the desired row took only 252 ms, which is an impressive 65 times faster than the previous approach. When comparing the row index between the two queries, we observe that the second query with <em>pyarrow</em> returned a DataFrame containing only the specific row with an index of 0, that\u2019s because when we provide a filter predicate, the rows which do not match this filter will be removed from scanned data before transforming it to DataFrame. This not only makes the retrieval significantly faster but also results in more efficient utilization of system resources due to the reduced data processing and memory overhead.</p>\n<h4>What about not sorted\u00a0columns?</h4>\n<p>Pyarrow follows a specific process for data retrieval. Initially, it reads the footer metadata (file metadata) to determine the data\u2019s range, denoted as (file_min, file_max). If the desired value falls outside this range, Pyarrow swiftly skips reading the file and returns an empty result. On the other hand, if the value lies within the range, Pyarrow proceeds to iterate through the row groups. For each row group, it first reads the column statistics and checks whether the value falls within the range of values for that row group, denoted as (row_group_min, row_group_max). Based on this check, Pyarrow decides whether to scan the entire row group or skip it altogether.</p>\n<p>It\u2019s essential to note that the second query\u2019s performance is not solely attributed to this process. Other factors contributing to the fast query execution include Pyarrow\u2019s overall efficiency and the small dataset size. In cases where the dataset is relatively small, there is no need to deserialize a large Pyarrow result and load it into a Pandas dataframe, further enhancing the query performance.</p>\n<p>To illustrate this, we can conduct a filtering experiment on our dataset using the second column, which is not sorted. This process will showcase the efficiency and capability of Pyarrow in handling unsorted data. By applying the filtering operation on the unsorted column, we can observe how Pyarrow optimizes the query and efficiently retrieves the required information, showcasing its powerful performance even with unsorted\u00a0data:</p>\n<pre>for i in range(parquet_file.metadata.num_row_groups):<br>    user_id_col_stats = parquet_file.metadata.row_group(i).column(1).statistics<br>    print(f\"row group: {i}, num of rows: {user_id_col_stats.num_values}, min: {user_id_col_stats.min}, max: {user_id_col_stats.max}\")<br><br>Result:<br>row group: 0, num of rows: 5000000, min: -10000, max: 10000<br>row group: 1, num of rows: 5000000, min: -10000, max: 10000<br>row group: 2, num of rows: 5000000, min: -10000, max: 10000<br>row group: 3, num of rows: 5000000, min: -10000, max: 10000<br>row group: 4, num of rows: 5000000, min: -10000, max: 10000<br>row group: 5, num of rows: 5000000, min: -10000, max: 10000<br>row group: 6, num of rows: 5000000, min: -10000, max: 10000<br>row group: 7, num of rows: 5000000, min: -10000, max: 10000<br>row group: 8, num of rows: 5000000, min: -10000, max: 10000<br>row group: 9, num of rows: 5000000, min: -10000, max: 10000<br>row group: 10, num of rows: 5000000, min: -10000, max: 10000<br>row group: 11, num of rows: 5000000, min: -10000, max: 10000<br>row group: 12, num of rows: 5000000, min: -10000, max: 10000<br>row group: 13, num of rows: 5000000, min: -10000, max: 10000<br>row group: 14, num of rows: 5000000, min: -10000, max: 10000<br>row group: 15, num of rows: 5000000, min: -10000, max: 10000<br>row group: 16, num of rows: 5000000, min: -10000, max: 10000<br>row group: 17, num of rows: 5000000, min: -10000, max: 10000<br>row group: 18, num of rows: 5000000, min: -10000, max: 10000<br>row group: 19, num of rows: 5000000, min: -10000, max: 10000</pre>\n<p>After examining the statistics of the second column, we discovered that all the row groups share the same minimum and maximum values. As a result, querying the file using any value within this range will not yield any advantage from utilizing these statistics:</p>\n<pre>%%time<br># apply the filter on the dataframe<br>pd.read_parquet(parquet_file_path).query(\"value == 6666\").count().to_dict()<br><br>Result:<br>CPU times: user 3.8 s, sys: 10.7 s, total: 14.5 s<br>Wall time: 26.4 s<br>{'user_id': 4994, 'value': 4994}<br><br>%%time<br># the filter to pyarrow<br>pd.read_parquet(parquet_file_path, filters=[(\"value\", \"=\", 6666)]).count().to_dict()<br><br>Result:<br><br>CPU times: user 3.02 s, sys: 1.28 s, total: 4.31 s<br>Wall time: 6.28 s<br>{'user_id': 4994, 'value': 4994}</pre>\n<p>Even without benefiting from the metadata, we observed a significant improvement in query performance by adding the predicate filter to Pyarrow. In fact, this approach resulted in the query being executed approximately three times faster compared to not pushing down the filter. This highlights the efficiency and effectiveness of Pyarrow in handling filters directly, showcasing its ability to enhance query speed even in scenarios where metadata statistics may not be fully exploited.</p>\n<h4>Filtering None\u00a0values</h4>\n<p>Most of the time, before processing our dataset, it is essential to filter out the None values. To achieve this, we typically load the file into a dataframe and then utilize the <em>dropna</em> method to eliminate the rows with None values. However, there is currently an issue with the pyarrow reader, as it does not yet support filtering None values directly. Nevertheless, there is a clever workaround we can\u00a0employ.</p>\n<p>In the following code, I demonstrate how to create a Parquet file with 2 columns, where approximately 50% of the values in the first column are set to\u00a0None:</p>\n<pre>parquet_file_path = \"another_test.parquet\"<br><br># Set the number of rows for the DataFrame<br>num_rows = 10**8 # 100M<br><br># Generate random data for the second column<br>second_column_data = np.random.rand(num_rows)<br><br># Create a mask to set 50% of the first column to None<br>mask = np.random.rand(num_rows) &lt; 0.5<br>first_column_data = np.where(mask, None, np.random.rand(num_rows))<br><br># Create the DataFrame<br>data = {\"Column1\": first_column_data, \"Column2\": second_column_data}<br>df = pd.DataFrame(data)<br><br># Write the result to a Parquet file with 20 row groups (5M records per row group)<br>df.to_parquet(parquet_file_path, index=False, row_group_size=5 * 10**6)</pre>\n<p>Loading the whole file and filtering the None values using pandas requires more than 20 seconds (~8s with dropping the None\u00a0values):</p>\n<pre>%%time<br><br>pd.read_parquet(parquet_file_path).sum().to_dict()<br><br>Result:<br>CPU times: user 3.96 s, sys: 3.78 s, total: 7.74 s<br>Wall time: 8.69 s<br>{'Column1': 24999424.127602533, 'Column2': 50000578.561534435}<br><br>%%time<br><br>pd.read_parquet(parquet_file_path).dropna(subset=[\"Column1\"]).sum().to_dict()<br><br>Result:<br>CPU times: user 4.99 s, sys: 15.5 s, total: 20.4 s<br>Wall time: 46.7 s<br>{'Column1': 24999424.127602667, 'Column2': 25001477.173362922}</pre>\n<p>To enable pyarrow to filter None values, we can utilize the filter \u2264 MAX_VALUEor \u2265 MIN_VALUE. In doing so, any rows with a None value in the filtered column will be excluded from the loaded\u00a0dataset:</p>\n<pre>%%time<br><br># Column1 type is DOUBLE, so max value is 2**53<br>pd.read_parquet(parquet_file_path, filters=[(\"Column1\", \"&lt;=\", 2**53)]).sum().to_dict()<br><br>Result:<br>CPU times: user 4.44 s, sys: 2.2 s, total: 6.64 s<br>Wall time: 5.83 s<br>{'Column1': 24999424.127602667, 'Column2': 25001477.173362922}</pre>\n<p>As observed, the result is obtained in just 6.64 seconds, which is 3 times faster than using the pandas <em>dropna</em>\u00a0method.</p>\n<h3>Summary</h3>\n<p>In conclusion, when working with Parquet files using pandas, it is crucial to leverage the power of pushing down the predicate filter to PyArrow to optimize performance and reduce memory usage. By utilizing PyArrow\u2019s support for various filters such as =, ==,\u00a0!=, &lt;, &gt;, &lt;=, &gt;=, in, and not in, we can efficiently add predicate filters to the Parquet metadata. This optimization allows us to load only the relevant data into memory, minimizing unnecessary reads and significantly improving overall processing speed. Embracing predicate filter in PyArrow ensures that we harness the full benefits of Parquet\u2019s metadata-driven storage, making data analysis and manipulation more efficient and scalable.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=595c9ee7112\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/munchy-bytes/are-you-using-parquet-with-pandas-in-the-right-way-595c9ee7112\">Are You Using Parquet with Pandas in the Right Way?</a> was originally published in <a href=\"https://medium.com/munchy-bytes\">Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","content":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*TAtJo4iw1SYbj-pt\"><figcaption>Photo by <a href=\"https://unsplash.com/@markfb?utm_source=medium&amp;utm_medium=referral\">Mark Fletcher-Brown</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><p>Apache Parquet is an open-source columnar storage format that is designed to efficiently store and process large amounts of structured data. It was developed as part of the Apache Hadoop ecosystem and is widely used in big data processing frameworks like Apache\u00a0Spark.</p>\n<h4>Why do we use\u00a0Parquet?</h4>\n<p>The Parquet format is specifically optimized for analytical workloads and is commonly used in data engineering and data science tasks. It provides several key benefits that make it an excellent choice for storing and processing data:</p>\n<ol>\n<li>\n<strong>Columnar Storage</strong>: Unlike row-based storage formats, Parquet organizes data in a columnar fashion, which means that all values of a single column are stored together. This layout allows for better compression and encoding of similar data, leading to reduced storage space and improved query performance.</li>\n<li>\n<strong>Compression</strong>: Parquet supports various compression algorithms, such as Snappy, Gzip, and Zstandard, which further contribute to reducing storage requirements and can speed up data reading from\u00a0disk.</li>\n<li>\n<strong>Schema Evolution</strong>: Parquet allows for schema evolution, meaning you can easily add, remove, or modify columns in the data without requiring a full rewrite of the dataset. This flexibility is particularly useful in scenarios where the schema evolves over\u00a0time.</li>\n<li>\n<strong>Cross-Platform Compatibility</strong>: Parquet is designed to be a portable format, enabling seamless data exchange between different systems and programming languages.</li>\n</ol>\n<h4>Why do we use Parquet with\u00a0pandas?</h4>\n<p>Pandas is a popular Python library for data manipulation and analysis. When working with large datasets in pandas, the performance of traditional file formats like CSV or JSON can be suboptimal due to their inherent limitations, such as row-based storage and lack of compression.</p>\n<p>By using Parquet files with pandas, you can take advantage of the benefits provided by the columnar storage format. Reading data from Parquet files into pandas DataFrames can be significantly faster compared to row-based formats, especially when dealing with large datasets. Additionally, since pandas natively supports Parquet, it can efficiently leverage the format\u2019s compression and encoding techniques, resulting in reduced memory usage and faster data processing.</p>\n<p>Moreover, Parquet\u2019s schema evolution feature aligns well with pandas\u2019 flexibility, allowing you to easily handle data with varying column structures or add new columns to existing DataFrames without much overhead.</p>\n<h3>Parquet file structure</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/601/1*L9O-Lj4V-QcsMAUpvs4tcA.gif\"><figcaption>Apache Parquet file structure</figcaption></figure><p>At its core, a Parquet file is composed of a collection of <strong>row groups</strong>, and each row group contains a set of <strong>column chunks</strong>. These column chunks store the actual data for each column of the\u00a0dataset.</p>\n<h4>Magic Number:</h4>\n<p>The magic number is a sequence of bytes at the beginning of a Parquet file (in the header), that identifies it as a Parquet file format. It is a fixed set of bytes (PAR1) that all Parquet files must begin with, allowing software to quickly identify whether a file is in the Parquet\u00a0format.</p>\n<h4>Row Groups</h4>\n<p>A Parquet file is divided into one or more row groups. Each row group is essentially a horizontal partition of the dataset, containing a specific number of rows. This design enables efficient read and write operations, as it allows applications to read only the required row groups instead of scanning the entire\u00a0file.</p>\n<h4>Pages</h4>\n<p>Within each row group, data is further divided into pages. Pages are the smallest unit of read and write in Parquet files. They represent a contiguous set of values for a particular column. Pages can be compressed independently, allowing for efficient compression and decompression.</p>\n<h4>Column Statistics</h4>\n<p>For every column chunk within a row group, Parquet can store column statistics. These statistics provide summary information about the data in each column, such as minimum and maximum values, the number of nulls, and other relevant data characteristics. Storing column statistics is beneficial for query optimization, as it allows Parquet readers to skip unnecessary row groups during data scanning, thus improving query performance.</p>\n<h4>Footer</h4>\n<p>The footer is located at the end of the Parquet file and contains essential metadata about the file\u2019s structure. It includes information about the schema of the dataset, the compression algorithms used for each column chunk, and the data location of each row group. The footer provides a high-level overview of the file, enabling quick access to critical metadata for processing the Parquet\u00a0file.</p>\n<h3>Processing Parquet files using\u00a0pandas</h3>\n<p>When working with Parquet files in pandas, you have the flexibility to choose between two engines: <em>fastparquet</em> and <em>pyarrow</em>. Both engines are third-party libraries that provide support for reading and writing Parquet files, and pandas seamlessly integrates with them to offer enhanced performance and efficiency.</p>\n<p><em>Fastparquet</em> is a popular Python library optimized for fast reading and writing of Parquet files. It is known for its speed and low memory footprint, making it an excellent choice for working with large datasets.</p>\n<p><em>Pyarrow</em> is part of the Apache Arrow project and is designed to provide efficient data interchange between different systems and languages. It offers seamless integration with pandas, allowing for fast and optimized data processing with Parquet\u00a0files.</p>\n<p>In this section, I will explain how to predicate your filters to <em>pyarrow</em> to reduce the size of the dataset processed by pandas, in order to reduce the processing time and the resource consumption.</p>\n<h4>Creating a simple file for\u00a0testing</h4>\n<pre>import pandas as pd<br>import numpy as np<br><br>parquet_file_path = \"test_data.parquet\"<br><br># Number of rows to generate<br>num_rows = 10**8 # 100M<br><br># Generate the DataFrame<br>data = {<br>    \"user_id\": np.arange(num_rows),<br>    \"value\": np.random.randint(-10000, 10001, size=num_rows)<br>}<br>df = pd.DataFrame(data)<br><br># Write the result to a Parquet file with 20 row groups (5M records per row group)<br>df.to_parquet(parquet_file_path, index=False, row_group_size=5 * 10**6)</pre>\n<p>The above script creates a single Parquet file sorted by <em>user_id</em>, consists of 20 row groups, each row group contains 5 millions rows. You can check the row groups metadata using pyarrow parquet\u00a0module:</p>\n<pre>import pyarrow.parquet as pq<br><br>parquet_file = pq.ParquetFile(parquet_file_path)<br><br>for i in range(parquet_file.metadata.num_row_groups):<br>    user_id_col_stats = parquet_file.metadata.row_group(i).column(0).statistics<br>    print(f\"row group: {i}, num of rows: {user_id_col_stats.num_values}, min: {user_id_col_stats.min}, max: {user_id_col_stats.max}\")</pre>\n<p>And here is the\u00a0output:</p>\n<pre>row group: 0, num of rows: 5000000, min: 0, max: 4999999<br>row group: 1, num of rows: 5000000, min: 5000000, max: 9999999<br>row group: 2, num of rows: 5000000, min: 10000000, max: 14999999<br>row group: 3, num of rows: 5000000, min: 15000000, max: 19999999<br>row group: 4, num of rows: 5000000, min: 20000000, max: 24999999<br>row group: 5, num of rows: 5000000, min: 25000000, max: 29999999<br>row group: 6, num of rows: 5000000, min: 30000000, max: 34999999<br>row group: 7, num of rows: 5000000, min: 35000000, max: 39999999<br>row group: 8, num of rows: 5000000, min: 40000000, max: 44999999<br>row group: 9, num of rows: 5000000, min: 45000000, max: 49999999<br>row group: 10, num of rows: 5000000, min: 50000000, max: 54999999<br>row group: 11, num of rows: 5000000, min: 55000000, max: 59999999<br>row group: 12, num of rows: 5000000, min: 60000000, max: 64999999<br>row group: 13, num of rows: 5000000, min: 65000000, max: 69999999<br>row group: 14, num of rows: 5000000, min: 70000000, max: 74999999<br>row group: 15, num of rows: 5000000, min: 75000000, max: 79999999<br>row group: 16, num of rows: 5000000, min: 80000000, max: 84999999<br>row group: 17, num of rows: 5000000, min: 85000000, max: 89999999<br>row group: 18, num of rows: 5000000, min: 90000000, max: 94999999<br>row group: 19, num of rows: 5000000, min: 95000000, max: 99999999</pre>\n<h4>Querying the file using\u00a0pandas</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*w7iGKXO549p3YHVV\"><figcaption>Photo by <a href=\"https://unsplash.com/@neonbrand?utm_source=medium&amp;utm_medium=referral\">Kenny Eliason</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><p>Typically, when working with Parquet files in pandas, the common approach involves loading the data into a pandas DataFrame and then performing the necessary data processing tasks on the loaded DataFrame:</p>\n<pre>%%time<br><br>pd.read_parquet(parquet_file_path).query(\"user_id == 8767068\")<br><br>CPU times: user 4.07 s, sys: 12.5 s, total: 16.6 s<br>Wall time: 31.6 s<br><br>&gt; {'user_id': {8767068: 8767068}, 'value': {8767068: 5893}}</pre>\n<p>As observed, the process of scanning and loading the entire Parquet file into a pandas DataFrame, followed by filtering to obtain matched rows, took approximately 16 seconds. However, pandas does not inherently pushdown filters to the Parquet engine, and it does not fully utilize the benefits of Parquet metadata.</p>\n<p>To optimize the process, we can manually pushdown our filters to the <em>pyarrow</em> engine, by adding a predicate filter to the<em> </em>reading<em> </em>method, leveraging the file metadata and <em>pyarrow</em>\u2019s processing optimizations. By doing so, we can significantly improve the query performance and reduce the processing time for data filtering and retrieval:</p>\n<pre>%%time<br><br>pd.read_parquet(parquet_file_path, filters=[(\"user_id\", \"=\", 8767068)]).to_dict()<br><br>CPU times: user 132 ms, sys: 120 ms, total: 252 ms<br>Wall time: 868 ms<br><br>&gt; {'user_id': {0: 8767068}, 'value': {0: 5893}}</pre>\n<p>In the last query, locating the desired row took only 252 ms, which is an impressive 65 times faster than the previous approach. When comparing the row index between the two queries, we observe that the second query with <em>pyarrow</em> returned a DataFrame containing only the specific row with an index of 0, that\u2019s because when we provide a filter predicate, the rows which do not match this filter will be removed from scanned data before transforming it to DataFrame. This not only makes the retrieval significantly faster but also results in more efficient utilization of system resources due to the reduced data processing and memory overhead.</p>\n<h4>What about not sorted\u00a0columns?</h4>\n<p>Pyarrow follows a specific process for data retrieval. Initially, it reads the footer metadata (file metadata) to determine the data\u2019s range, denoted as (file_min, file_max). If the desired value falls outside this range, Pyarrow swiftly skips reading the file and returns an empty result. On the other hand, if the value lies within the range, Pyarrow proceeds to iterate through the row groups. For each row group, it first reads the column statistics and checks whether the value falls within the range of values for that row group, denoted as (row_group_min, row_group_max). Based on this check, Pyarrow decides whether to scan the entire row group or skip it altogether.</p>\n<p>It\u2019s essential to note that the second query\u2019s performance is not solely attributed to this process. Other factors contributing to the fast query execution include Pyarrow\u2019s overall efficiency and the small dataset size. In cases where the dataset is relatively small, there is no need to deserialize a large Pyarrow result and load it into a Pandas dataframe, further enhancing the query performance.</p>\n<p>To illustrate this, we can conduct a filtering experiment on our dataset using the second column, which is not sorted. This process will showcase the efficiency and capability of Pyarrow in handling unsorted data. By applying the filtering operation on the unsorted column, we can observe how Pyarrow optimizes the query and efficiently retrieves the required information, showcasing its powerful performance even with unsorted\u00a0data:</p>\n<pre>for i in range(parquet_file.metadata.num_row_groups):<br>    user_id_col_stats = parquet_file.metadata.row_group(i).column(1).statistics<br>    print(f\"row group: {i}, num of rows: {user_id_col_stats.num_values}, min: {user_id_col_stats.min}, max: {user_id_col_stats.max}\")<br><br>Result:<br>row group: 0, num of rows: 5000000, min: -10000, max: 10000<br>row group: 1, num of rows: 5000000, min: -10000, max: 10000<br>row group: 2, num of rows: 5000000, min: -10000, max: 10000<br>row group: 3, num of rows: 5000000, min: -10000, max: 10000<br>row group: 4, num of rows: 5000000, min: -10000, max: 10000<br>row group: 5, num of rows: 5000000, min: -10000, max: 10000<br>row group: 6, num of rows: 5000000, min: -10000, max: 10000<br>row group: 7, num of rows: 5000000, min: -10000, max: 10000<br>row group: 8, num of rows: 5000000, min: -10000, max: 10000<br>row group: 9, num of rows: 5000000, min: -10000, max: 10000<br>row group: 10, num of rows: 5000000, min: -10000, max: 10000<br>row group: 11, num of rows: 5000000, min: -10000, max: 10000<br>row group: 12, num of rows: 5000000, min: -10000, max: 10000<br>row group: 13, num of rows: 5000000, min: -10000, max: 10000<br>row group: 14, num of rows: 5000000, min: -10000, max: 10000<br>row group: 15, num of rows: 5000000, min: -10000, max: 10000<br>row group: 16, num of rows: 5000000, min: -10000, max: 10000<br>row group: 17, num of rows: 5000000, min: -10000, max: 10000<br>row group: 18, num of rows: 5000000, min: -10000, max: 10000<br>row group: 19, num of rows: 5000000, min: -10000, max: 10000</pre>\n<p>After examining the statistics of the second column, we discovered that all the row groups share the same minimum and maximum values. As a result, querying the file using any value within this range will not yield any advantage from utilizing these statistics:</p>\n<pre>%%time<br># apply the filter on the dataframe<br>pd.read_parquet(parquet_file_path).query(\"value == 6666\").count().to_dict()<br><br>Result:<br>CPU times: user 3.8 s, sys: 10.7 s, total: 14.5 s<br>Wall time: 26.4 s<br>{'user_id': 4994, 'value': 4994}<br><br>%%time<br># the filter to pyarrow<br>pd.read_parquet(parquet_file_path, filters=[(\"value\", \"=\", 6666)]).count().to_dict()<br><br>Result:<br><br>CPU times: user 3.02 s, sys: 1.28 s, total: 4.31 s<br>Wall time: 6.28 s<br>{'user_id': 4994, 'value': 4994}</pre>\n<p>Even without benefiting from the metadata, we observed a significant improvement in query performance by adding the predicate filter to Pyarrow. In fact, this approach resulted in the query being executed approximately three times faster compared to not pushing down the filter. This highlights the efficiency and effectiveness of Pyarrow in handling filters directly, showcasing its ability to enhance query speed even in scenarios where metadata statistics may not be fully exploited.</p>\n<h4>Filtering None\u00a0values</h4>\n<p>Most of the time, before processing our dataset, it is essential to filter out the None values. To achieve this, we typically load the file into a dataframe and then utilize the <em>dropna</em> method to eliminate the rows with None values. However, there is currently an issue with the pyarrow reader, as it does not yet support filtering None values directly. Nevertheless, there is a clever workaround we can\u00a0employ.</p>\n<p>In the following code, I demonstrate how to create a Parquet file with 2 columns, where approximately 50% of the values in the first column are set to\u00a0None:</p>\n<pre>parquet_file_path = \"another_test.parquet\"<br><br># Set the number of rows for the DataFrame<br>num_rows = 10**8 # 100M<br><br># Generate random data for the second column<br>second_column_data = np.random.rand(num_rows)<br><br># Create a mask to set 50% of the first column to None<br>mask = np.random.rand(num_rows) &lt; 0.5<br>first_column_data = np.where(mask, None, np.random.rand(num_rows))<br><br># Create the DataFrame<br>data = {\"Column1\": first_column_data, \"Column2\": second_column_data}<br>df = pd.DataFrame(data)<br><br># Write the result to a Parquet file with 20 row groups (5M records per row group)<br>df.to_parquet(parquet_file_path, index=False, row_group_size=5 * 10**6)</pre>\n<p>Loading the whole file and filtering the None values using pandas requires more than 20 seconds (~8s with dropping the None\u00a0values):</p>\n<pre>%%time<br><br>pd.read_parquet(parquet_file_path).sum().to_dict()<br><br>Result:<br>CPU times: user 3.96 s, sys: 3.78 s, total: 7.74 s<br>Wall time: 8.69 s<br>{'Column1': 24999424.127602533, 'Column2': 50000578.561534435}<br><br>%%time<br><br>pd.read_parquet(parquet_file_path).dropna(subset=[\"Column1\"]).sum().to_dict()<br><br>Result:<br>CPU times: user 4.99 s, sys: 15.5 s, total: 20.4 s<br>Wall time: 46.7 s<br>{'Column1': 24999424.127602667, 'Column2': 25001477.173362922}</pre>\n<p>To enable pyarrow to filter None values, we can utilize the filter \u2264 MAX_VALUEor \u2265 MIN_VALUE. In doing so, any rows with a None value in the filtered column will be excluded from the loaded\u00a0dataset:</p>\n<pre>%%time<br><br># Column1 type is DOUBLE, so max value is 2**53<br>pd.read_parquet(parquet_file_path, filters=[(\"Column1\", \"&lt;=\", 2**53)]).sum().to_dict()<br><br>Result:<br>CPU times: user 4.44 s, sys: 2.2 s, total: 6.64 s<br>Wall time: 5.83 s<br>{'Column1': 24999424.127602667, 'Column2': 25001477.173362922}</pre>\n<p>As observed, the result is obtained in just 6.64 seconds, which is 3 times faster than using the pandas <em>dropna</em>\u00a0method.</p>\n<h3>Summary</h3>\n<p>In conclusion, when working with Parquet files using pandas, it is crucial to leverage the power of pushing down the predicate filter to PyArrow to optimize performance and reduce memory usage. By utilizing PyArrow\u2019s support for various filters such as =, ==,\u00a0!=, &lt;, &gt;, &lt;=, &gt;=, in, and not in, we can efficiently add predicate filters to the Parquet metadata. This optimization allows us to load only the relevant data into memory, minimizing unnecessary reads and significantly improving overall processing speed. Embracing predicate filter in PyArrow ensures that we harness the full benefits of Parquet\u2019s metadata-driven storage, making data analysis and manipulation more efficient and scalable.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=595c9ee7112\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/munchy-bytes/are-you-using-parquet-with-pandas-in-the-right-way-595c9ee7112\">Are You Using Parquet with Pandas in the Right Way?</a> was originally published in <a href=\"https://medium.com/munchy-bytes\">Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","enclosure":{},"categories":["data-science","pyarrow","apache-parquet","data-analysis","pandas"]},{"title":"Airflow: Scalable and Cost-Effective Architecture","pubDate":"2023-07-03 07:11:38","link":"https://medium.com/apache-airflow/airflow-scalable-and-cost-effective-architecture-8edb4f8aed65?source=rss-8725a0967242------2","guid":"https://medium.com/p/8edb4f8aed65","author":"Hussein Awala","thumbnail":"","description":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*FJsMPN5kPMI7JuqhsaP7rA.png\"></figure><p>Airflow is a popular open-source platform for orchestrating and scheduling complex data workflows. It provides a flexible and scalable framework that enables users to define, manage, and monitor their data pipelines with ease. One of the key features of Airflow is its ability to support multiple executors, each offering distinct benefits and trade-offs based on specific use cases and infrastructure requirements.</p>\n<p>In this article, we\u2019ll explore three prominent executors in Airflow: LocalExecutor, CeleryExecutor, and KubernetesExecutor, and I\u2019ll introduce one of the best architecture for big projects to reduce the cost and the resources usage.</p>\n<h3>Airflow Executors</h3>\n<p>Airflow offers support for two types of executors: local (SequentialExecutor and LocalExecutor) and remote (CeleryExecutor, KubernetesExecutor, CeleryKubernetesExecutor, DaskExecutor and LocalKubernetesExecutor). Choosing the most suitable executor depends on the specific requirements and infrastructure setup of your Airflow deployment.</p>\n<p>Let\u2019s explore the three most commonly used executors in\u00a0Airflow:</p>\n<p><strong>LocalExecutor</strong>: The LocalExecutor is suitable for small-scale deployments or environments where distributed computing is not required. It operates by running task instances in parallel on a single machine, by spawning processes in a controlled fashion. While it lacks the distributed nature of other executors, it offers simplicity and ease of setup, making it a good choice for local development and testing scenarios.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/530/1*vgaczwXeuiDgcxR3WyqAIw.png\"><figcaption>Local Executor</figcaption></figure><p><strong>CeleryExecutor</strong>: The CeleryExecutor is a widely-used executor in Airflow, known for its scalability and ability to handle large-scale distributed workloads. It leverages Celery, a distributed task queue system, to parallelize task execution across multiple worker nodes. With CeleryExecutor, tasks can be distributed to workers running on different machines or even across multiple clusters, enabling efficient utilization of resources and high throughput. This executor is recommended for deployments that require scalability, fault tolerance, and efficient resource management.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/700/1*0oxQgeujzl_OAUEICKDH-A.png\"><figcaption>Celery Executor</figcaption></figure><p><strong>KubernetesExecutor</strong>: The KubernetesExecutor is specifically designed for running Airflow on Kubernetes clusters, leveraging the powerful orchestration capabilities provided by Kubernetes. With this executor, Airflow spins up a separate pod for each task, enabling isolation, fault tolerance, and seamless integration with other Kubernetes resources. It offers scalability and elasticity, allowing for efficient resource allocation and auto-scaling based on demand. The KubernetesExecutor is well-suited for deployments in cloud-native environments that heavily rely on containerization and Kubernetes for managing resources.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/700/1*BsiwJA56KQDm2khWkdkLEQ.png\"></figure><h3>Scalability in\u00a0Airflow</h3>\n<p>The <strong>KubernetesExecutor</strong> in Airflow offers seamless scalability without requiring manual setup, as Kubernetes handles the scaling process entirely. By using this executor, you only need to focus on managing the scheduler by creating one or multiple scheduler instances based on your workload. Airflow will automatically generate separate worker pods for each task, providing a highly flexible cluster size and ensuring exceptional scalability.</p>\n<p>However, there are two significant drawbacks to consider with this executor:</p>\n<ol>\n<li>\n<strong>Resource Overhead due to Pod-per-Task Approach</strong>: One significant challenge of using Airflow with the KubernetesExecutor is the inherent resource overhead. With this executor, Airflow launches a separate pod for each task, regardless of its size or resource requirements. While this approach provides isolation and fault tolerance, it can result in inefficient utilization of resources. Small tasks that could have been executed on a shared pod end up consuming a full pod\u2019s worth of resources, leading to\u00a0wastage.</li>\n<li>\n<strong>Increased Latency in Task Execution</strong>: Another limitation of the KubernetesExecutor is the latency introduced during task execution. Since each task is assigned to a separate pod, the orchestration overhead associated with pod creation and initialization adds to the overall latency. Even for small and quick tasks, the time taken for pod startup and teardown can become a significant bottleneck.</li>\n</ol>\n<h4>What about CeleryExecutor?</h4>\n<p>Although the <strong>CeleryExecutor</strong> provides distributed task execution in Airflow, scaling the Celery cluster itself can be a challenge. However, by deploying Celery on Kubernetes and utilizing Kubernetes-based Event Driven Autoscaling (KEDA), we can overcome this limitation and achieve dynamic scaling based on specific conditions. One such condition could be the number of queued tasks in\u00a0Airflow.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/820/1*f726_zVaX7rf9KQ06HInqw.png\"><figcaption>CeleryExecutor with\u00a0KEDA</figcaption></figure><p>KEDA integrates seamlessly with Kubernetes and provides an autoscaling solution that takes into account various metrics, including external event sources, to trigger scaling actions. By combining the power of Celery, Kubernetes, and KEDA, Airflow users can achieve efficient and dynamic scaling of their CeleryExecutor clusters, ensuring optimal performance and resource utilization for their data workflows.</p>\n<p>By leveraging Kubernetes and KEDA, we can configure rules that monitor the number of queued tasks in Airflow. When the number of queued tasks exceeds a certain threshold, KEDA can automatically scale up the number of Celery workers by adding additional worker pods to the cluster. This dynamic scaling ensures that the cluster adapts to the workload demands, enabling efficient resource utilization and improved task processing times.</p>\n<p>If you are using the official helm chart to install Airflow on Kubernetes, you can easily activate KEDA by adding these configurations to the values\u00a0file:</p>\n<pre>workers:<br>  keda:<br>    enabled: true<br><br>    # How often KEDA polls the airflow DB to report new scale requests to the HPA<br>    pollingInterval: 5<br><br>    # Minimum number of workers created by keda<br>    minReplicaCount: 4<br><br>    # Maximum number of workers created by keda<br>    maxReplicaCount: 10</pre>\n<p>Airflow uses the following query in the CeleryExecutor to determine the appropriate number of workers for the\u00a0cluster:</p>\n<pre>SELECT ceil(COUNT(*)::decimal / {{ .Values.config.celery.worker_concurrency }})<br>FROM task_instance<br>WHERE (state='running' OR state='queued')</pre>\n<p>However, this approach can lead to instability in the number of workers, especially with small tasks and sensors in \u201creschedule mode\u201d. In such case, KEDA may increase the number of replicas as soon as a queued task is waiting for a worker, without waiting for the running tasks to finish. This can result in the creation and initialization of new pods, and potentially, a new node would be added to the cluster by the node auto-scaler. However, there may be a latency of 1 to 3 minutes between the scaling decision and the worker being ready to execute the tasks. In the meantime, some of the other tasks might have already finished, and the queued tasks could have been executed by other workers. As a result, KEDA might decide to reduce the number of replicas, leading to the addition of a new node that is running idle for a\u00a0while.</p>\n<h4>Better KEDA queries for some use\u00a0cases</h4>\n<p>To ensure that the current workers have a chance to execute the queued tasks, we can modify the query by adding a filter based on the queued time. This filter will consider only the running tasks and the tasks that have been queued for more than X\u00a0minutes:</p>\n<pre>SELECT ceil(COUNT(*)::decimal / {{ .Values.config.celery.worker_concurrency }})<br>FROM task_instance<br>WHERE state='running' <br>  OR (state='queued' AND queued_dttm &lt; NOW() - INTERVAL '1 minute');</pre>\n<p>By implementing this filter, we introduce a delay of X minutes in the scaling-up decision, allowing the current workers to process the queued tasks. Consequently, the overall execution time of the tasks will also include an additional X minutes in such scenarios where a scale-up is genuinely required.</p>\n<p>To prioritize critical tasks over less important ones without introducing extra latency to the critical tasks, you can use a variant of the query based on the priority_weight and queued_dttm columns (you should provide priority_weight parameter and use <a href=\"https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/priority-weight.html\">absolute weighting method</a>):</p>\n<pre>SELECT ceil(COUNT(*)::decimal / {{ .Values.config.celery.worker_concurrency }})<br>FROM task_instance<br>WHERE state = 'running'<br>  OR (<br>    state = 'queued' AND (<br>      priority_weight &gt;= 5<br>      OR queued_dttm &lt; NOW() - INTERVAL ((5 - priority_weight) || ' minute')<br>    )<br>  );</pre>\n<p>In this query, a conditional statement (priority_weight &gt;= 5) is used to determine if the dynamic interval should be applied. If the priority_weight is greater than or equal to 5 (indicating a critical task), the first condition (priority_weight &gt;= 5) is satisfied, and the second condition queued_dttm &lt; NOW() - INTERVAL '1 minute'is not considered.</p>\n<p>However, if the priority_weight is less than 5, the first condition is not satisfied, and the second condition is evaluated. In this case, the interval is calculated as (5 - priority_weight) concatenated with the string \u2018 minute\u2019. This calculation allows for some tolerance based on the task criticality.</p>\n<p>Please note that updating the KEDA query is not currently possible in the official helm chart, but it will be feasible in the next release 1.11.0 (<a href=\"https://github.com/apache/airflow/pull/32308\">link to\u00a0PR</a>).</p>\n<h4>Scaling with long-running tasks</h4>\n<p>One important consideration when using workers auto-scaling with a mix of long and short running tasks is how the scaling behavior can impact the execution of long-running tasks. Let\u2019s explore this scenario in more\u00a0details:</p>\n<p>In a situation where the tasks are scheduled hourly and comprise a mixture of long and short running tasks, the scaling behavior becomes crucial. Suppose the number of queued tasks is between 3 and 4 times the value of worker concurrency. Here\u2019s how the scaling process\u00a0unfolds:</p>\n<ol>\n<li>\n<strong>Scaling Up with KEDA</strong>: When the tasks start, KEDA helps drive the scaling up the workers to ensure all the tasks can run concurrently. In this case, KEDA might scale up the number of replicas to\u00a04.</li>\n<li>\n<strong>Scaling Down with HPA</strong>: As the short-running tasks complete, the Horizontal Pod Autoscaler (HPA) could make a decision to scale down the number of replicas from 4 to 2, as the workload decreases. However, in this scaling-down process, there is no control over which specific replicas get terminated.</li>\n<li>\n<strong>Impact on Long-Running Tasks</strong>: The scaling down of replicas can potentially impact long-running tasks that are still in progress. If a long-running task was running on one of the terminated replicas, it would be retried from the beginning on another available worker (that depends on the retry configuration of the task instance). This retry behavior occurs because the terminated replica\u2019s state is lost, and the task needs to be rescheduled on a different worker.</li>\n</ol>\n<h4>Leverage the container lifecycle</h4>\n<p>Kubernetes offers lifecycle hooks that can be utilized to delay the termination process. Consider a scenario where a worker, currently 40 minutes into executing a one-hour task, is scheduled for termination. Kubernetes sends a SIGTERM signal to indicate the intent to terminate the worker pod. However, instead of terminating immediately, the worker can receive and process this signal to delay the termination until the ongoing tasks are completed.</p>\n<p>By implementing the necessary logic within the lifecycle hooks, the worker pod can respond to the SIGTERM signal, indicating that it needs additional time to finish the current tasks. Kubernetes honors this request by waiting for either a response to the SIGTERM signal or until the specified terminationGracePeriodSeconds has elapsed before forcibly terminating the worker pod. This ensures that the ongoing tasks have an opportunity to complete before the worker pod is forcefully terminated, promoting a smoother shutdown process and minimizing disruptions.</p>\n<p>When CeleryExecutor is used in Airflow, Airflow creates Celery workers and treats them as Airflow workers, leading to handling the system signals such as SIGTERM by Celery. Here is the sequence of actions taken by the worker upon receiving a SIGTERM\u00a0signal:</p>\n<ol>\n<li>\n<strong>Unsubscribing from Queues</strong>: Upon receiving the SIGTERM signal, the worker immediately unsubscribes from all the queues it is currently listening to. This ensures that no new tasks are assigned to the worker during the termination process.</li>\n<li>\n<strong>Waiting for Task Completion</strong>: The worker then waits for all the tasks it is currently executing to finish. Despite the termination signal, the worker will continue to acknowledge the tasks as they are completed.</li>\n<li>\n<strong>Closing Connections and Exiting</strong>: Once all the tasks are complete, the worker proceeds to close all connections, stop any associated subprocesses, and then exits gracefully. This ensures that the worker terminates cleanly and releases any held resources.</li>\n</ol>\n<p>However, if the terminationGracePeriodSecondsis reached during this sequence, Kubernetes will forcibly terminate the worker. Therefore, it is essential to carefully configure this parameter based on the characteristics of your\u00a0tasks.</p>\n<p>This solution is effective for tasks with a maximum duration of one hour. Setting the terminationGracePeriodSeconds to a value greater than one hour can result in excessive resource wastage. As described in the first step of the actions sequence, the worker stops accepting new tasks upon receiving a SIGTERM signal. Therefore, configuring the terminationGracePeriodSeconds to a longer duration, such as 24 hours, would allocate the entire worker\u2019s resources to a single task, which is not desirable.</p>\n<h3>Don\u2019t worry, Airflow has the\u00a0solution</h3>\n<p>While Airflow is capable of running tasks on its workers, it primarily serves as a scheduler for managing workflows. In many cases, the heavy lifting of executing massive tasks is offloaded to external systems (Docker, Kubernetes, Spark, Trino, Athena, Snowflake,\u00a0\u2026). Airflow tasks often involve checking the state of processing on these external systems and waiting to receive the\u00a0results.</p>\n<p>To optimize resource utilization and minimize idle operators or sensors, Airflow introduced a core component called the Triggerer. The <strong>Triggerer</strong> allows lightweight checks to be performed instead of running the task or sensor continuously on a worker and occupying a worker slot. Instead, these tasks can suspend themselves when they know they need to wait and delegate the responsibility of resuming them to a\u00a0Trigger.</p>\n<p><strong>Triggers</strong> are small, asynchronous pieces of Python code designed to be executed together in a single Python process. Their asynchronous nature enables them to coexist efficiently. Here\u2019s an overview of how this process\u00a0works:</p>\n<ol>\n<li>\n<strong>Task Deferral</strong>: When a task instance reaches a point where it needs to wait, it defers itself using a trigger associated with the event that should resume it. By deferring, the worker is freed up to execute other\u00a0tasks.</li>\n<li>\n<strong>Registration and Execution</strong>: The new Trigger instance is registered within Airflow and picked up by a triggerer process, which is responsible for handling triggers.</li>\n<li>\n<strong>Trigger Execution</strong>: The triggerer process runs the trigger until it reaches the firing condition specified. Once the trigger fires, it triggers the rescheduling of the associated source\u00a0task.</li>\n<li>\n<strong>Task Resumption</strong>: The scheduler then adds the task to the queue to be executed on a worker node, ensuring its execution resumes when resources are available.</li>\n</ol>\n<p>Implementing a trigger for your operator is typically not a complex task. However, it relies on the availability of clients or libraries that facilitate asynchronous communication with the external system. The Airflow community, including committers and contributors, is actively working on expanding the support for the deferrable mode across a wide range of operators provided by the community.</p>\n<p>To illustrate how to implement a trigger in custom operators, here\u2019s an\u00a0example:</p>\n<pre>class ExampleOperator(BaseOperator):<br>  def execute(self, context):<br>    # run the task on a external system<br>    ...<br>    # then defer it to create the trigger and leave the worker slot<br>    self.defer(<br>      trigger=ExampleTrigger(),<br>      method_name=\"resume_execution\"<br>    )<br><br>  def resume_execution(self, context: dict, event: dict):<br>    # resume the execution<br><br>class ExampleTrigger(BaseTrigger):<br>  def serialize(self):<br>    return (\"package.module.ExampleTrigger\", {})<br><br>  async def run(self):<br>    step = 1 # number of seconds to wait between checks<br>    while True:<br>      # async check with the external system<br>      if not finished:<br>        await asyncio.sleep(step)<br>      else:<br>        yield TriggerEvent({<br>          # put anything you want to send to the resume_execution method<br>        })<br>        return</pre>\n<h4>Triggerers availability</h4>\n<p>Triggers are designed to be highly available, allowing us to run multiple instances of the triggerer on different hosts. These instances utilize DB locking to coexist, similar to the schedulers.</p>\n<p>Since the triggers consist of lightweight and asynchronous Python code, by default, each triggerer runs 1000 triggers simultaneously. However, we have the flexibility to increase this capacity to tens of thousands or decrease it to hundreds, depending on the workload and specific triggers.</p>\n<p>Every second, the triggerer attempts to load additional triggers, provided that its capacity allows for it. It only loads triggers that are not assigned to any triggerer or are assigned to a non-responsive one. To detect non-responsive triggerers, the triggerer retrieves the last heartbeat timestamp for each one and checks if it is older than trigger.job_heartbeat_sec (a new configuration will be introduced in Airflow 2.6.3 through this\u00a0<a href=\"https://github.com/apache/airflow/pull/32123\">PR</a>).</p>\n<p>It's important to note that a delayed heartbeat doesn't necessarily mean that the triggerer has died; it could be due to temporary network issues that prevent the heartbeats from being sent. In such cases, the same trigger might end up running on multiple triggerers simultaneously. However, this is 100% safe as long as the trigger's responsibility is respected, which involves waiting for an external task to finish. Airflow will process the first sent event to resume the task and ignore the duplicates. Therefore, it's crucial to consider that Airflow provides an at-least-once guarantee for triggers, rather than an exactly-once guarantee.</p>\n<h4>KEDA for triggerers</h4>\n<p>As we can observe, workers and triggerers share certain characteristics. Both can execute a limited number of tasks/triggers at once, and the number of instances of each service should be determined based on the workload. For that, it would be beneficial to utilize KEDA to scale the triggerers\u2019 deployment, just as we do for the\u00a0workers.</p>\n<p>However, there is a difference in behavior between the triggerers and the workers when it comes to handling killed instances. The triggerers, being fully fault-tolerant, do not require the same level of concern as the workers. In the case where KEDA decides to reduce the number of triggerers and randomly kill one, the remaining triggerers will wait for trigger.job_heartbeat_sec before assuming responsibility for the affected triggers. They will then proceed to adopt and run them, then enter the checking loop and monitor the state of the external tasks. This ensures that the triggers continue to function properly despite the intermittent loss of triggerer instances.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*t6YNd6w1dUGJ5eZyVvPdqw.png\"><figcaption>CeleryExecutor and Triggerers with\u00a0KEDA</figcaption></figure><p>In version 1.11.0 of the Airflow Helm chart (link to PR), the integration of KEDA with triggerers will be introduced (<a href=\"https://github.com/apache/airflow/pull/32302\">link to PR</a>). By default, the chart will use the following query for\u00a0KEDA:</p>\n<pre>SELECT ceil(COUNT(*)::decimal / {{ .Values.config.triggerer.default_capacity }})<br>FROM trigger</pre>\n<p>However, you will have the flexibility to provide your own query. For instance, if you want KEDA to wait for a minute before scaling up, you can return the count of assigned triggers and the count of unassigned triggers older than one minute. Here is an example of the Helm values configuration:</p>\n<pre>triggerer:<br>  enabled: true<br>  keda:<br>    enabled: true<br>    minReplicaCount: 0<br>    maxReplicaCount: 3<br>    query: &gt;-<br>      SELECT ceil(COUNT(*)::decimal / {{ .Values.config.triggerer.default_capacity }})<br>      FROM trigger<br>      WHERE triggerer_id IS NOT NULL OR created_date &lt; NOW() - INTERVAL '1 minute'</pre>\n<h4>KPO</h4>\n<p>As Airflow recommend running tasks on external systems, one such system that deserves highlighting is the Kubernetes cluster where Airflow is deployed. When dealing with tasks that require a long running time or extensive resource requirements, one of the optimal choices is to utilize the KubernetesPodOperator. This operator executes a Docker container within a Kubernetes pod, ensuring all the necessary resources are provisioned, including memory, CPU, volumes, and configurations stored in the Kubernetes database. By leveraging the KubernetesPodOperator, Airflow can efficiently manage and execute tasks with demanding resource needs within the Kubernetes cluster.</p>\n<p>While Airflow offers a custom executor that defaults to using CeleryExecutor and provides KubernetesExecutor as an on-demand option (CeleryKubernetesExecutor), my preferred combination is CeleryExecutor with KubernetesPodOperator (KPO). The integration of CeleryExecutor with KPO offers several advantages.</p>\n<p>With KPO, the created container is fully independent of Airflow, eliminating the need to install the Airflow Python library and reducing the image size. This independence simplifies testing and makes upgrades much easier. Additionally, KPO allows us the freedom to run any type of image and execute various types of\u00a0code.</p>\n<p>On the other hand, KubernetesExecutor creates an Airflow worker and is primarily focused on running Python code. It requires using the same Docker image for all tasks (or an image per group of tasks), resulting in larger image sizes, unnecessary dependencies, and complications with Airflow-related testing.</p>\n<p>This combination is particularly advantageous when running the operator in deferrable mode. The task instance simply creates the pod and defers itself, thereby delegating the responsibility of waiting to a lightweight trigger. This approach minimizes the resources required for waiting and allows for efficient utilization of the system. By using CeleryExecutor with KPO in deferrable mode, we can optimize task execution and enhance the overall performance of\u00a0Airflow.</p>\n<h3>The ideal architecture for big\u00a0projects</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*qNAVAYNkaczMNqjP95BIAA.png\"></figure><p>In summary, the <strong>CeleryExecutor</strong> stands out among other Airflow executors in terms of execution latency and resource management. Its integration with Kubernetes and KEDA allows for scalability based on workload. By setting terminationGracePeriodSeconds to 1 hour, most of the running tasks can avoid interruption when a scale-down decision is made by\u00a0KEDA.</p>\n<p>For tasks executed outside of Airflow, I recommend using the deferable mode to free up worker slots and scale down the cluster as needed. Triggers can be created to run on triggerers with low resource consumption. These triggerers are highly available and fault-tolerant. To adapt to varying workload sizes, especially in projects with hourly or daily peaks, we explored how to use KEDA to scale triggerers without worrying about their execution.</p>\n<p>Additionally, I highlight the benefits of using the KubernetesPodOperator as the preferred method for executing long-running or resource-intensive tasks. It surpasses the use of the CeleryKubernetesExecutor due to the lightweight used images, flexibility in choosing images and applications, ease of testing, and simplicity in upgrading Airflow versions.</p>\n<p>This architecture is suitable for projects of any size but requires knowledge of various technologies such as Celery, Kubernetes, and KEDA. It is particularly recommended for larger projects with intermittent peaks where reducing execution latency \u26a1\ufe0f, infrastructure costs \ud83d\udcb8, and resource consumption \ud83c\udf31are the key\u00a0goals.</p>\n<h4>Next Articles</h4>\n<p>In the upcoming articles related to the Airflow topic, we will dive deeper into various aspects of Airflow optimization and design. Here\u2019s a glimpse of what you can\u00a0expect:</p>\n<ol>\n<li>\n<strong>Optimizing Scheduler Performance</strong>: Learn valuable techniques and best practices to fine-tune the performance of the Airflow scheduler. Discover ways to optimize resource utilization and reduce scheduling latency for your workflows.</li>\n<li>\n<strong>Designing Performant DAGs</strong>: Explore strategies for creating efficient DAGs in Airflow. We\u2019ll discuss approaches to minimize redundancy, improve task parallelism, and ensure smooth execution of your workflows.</li>\n<li>\n<strong>Building Dynamic DAGs and Tasks</strong>: Harness the full power of Airflow\u2019s dynamic capabilities. Discover how to dynamically generate DAGs and tasks based on runtime conditions, enabling you to create flexible and scalable workflows.</li>\n</ol>\n<p>Stay tuned for these upcoming articles as we unlock advanced Airflow concepts and empower you to take your data orchestration to the next\u00a0level!</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=8edb4f8aed65\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/apache-airflow/airflow-scalable-and-cost-effective-architecture-8edb4f8aed65\">Airflow: Scalable and Cost-Effective Architecture</a> was originally published in <a href=\"https://medium.com/apache-airflow\">Apache Airflow</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","content":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*FJsMPN5kPMI7JuqhsaP7rA.png\"></figure><p>Airflow is a popular open-source platform for orchestrating and scheduling complex data workflows. It provides a flexible and scalable framework that enables users to define, manage, and monitor their data pipelines with ease. One of the key features of Airflow is its ability to support multiple executors, each offering distinct benefits and trade-offs based on specific use cases and infrastructure requirements.</p>\n<p>In this article, we\u2019ll explore three prominent executors in Airflow: LocalExecutor, CeleryExecutor, and KubernetesExecutor, and I\u2019ll introduce one of the best architecture for big projects to reduce the cost and the resources usage.</p>\n<h3>Airflow Executors</h3>\n<p>Airflow offers support for two types of executors: local (SequentialExecutor and LocalExecutor) and remote (CeleryExecutor, KubernetesExecutor, CeleryKubernetesExecutor, DaskExecutor and LocalKubernetesExecutor). Choosing the most suitable executor depends on the specific requirements and infrastructure setup of your Airflow deployment.</p>\n<p>Let\u2019s explore the three most commonly used executors in\u00a0Airflow:</p>\n<p><strong>LocalExecutor</strong>: The LocalExecutor is suitable for small-scale deployments or environments where distributed computing is not required. It operates by running task instances in parallel on a single machine, by spawning processes in a controlled fashion. While it lacks the distributed nature of other executors, it offers simplicity and ease of setup, making it a good choice for local development and testing scenarios.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/530/1*vgaczwXeuiDgcxR3WyqAIw.png\"><figcaption>Local Executor</figcaption></figure><p><strong>CeleryExecutor</strong>: The CeleryExecutor is a widely-used executor in Airflow, known for its scalability and ability to handle large-scale distributed workloads. It leverages Celery, a distributed task queue system, to parallelize task execution across multiple worker nodes. With CeleryExecutor, tasks can be distributed to workers running on different machines or even across multiple clusters, enabling efficient utilization of resources and high throughput. This executor is recommended for deployments that require scalability, fault tolerance, and efficient resource management.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/700/1*0oxQgeujzl_OAUEICKDH-A.png\"><figcaption>Celery Executor</figcaption></figure><p><strong>KubernetesExecutor</strong>: The KubernetesExecutor is specifically designed for running Airflow on Kubernetes clusters, leveraging the powerful orchestration capabilities provided by Kubernetes. With this executor, Airflow spins up a separate pod for each task, enabling isolation, fault tolerance, and seamless integration with other Kubernetes resources. It offers scalability and elasticity, allowing for efficient resource allocation and auto-scaling based on demand. The KubernetesExecutor is well-suited for deployments in cloud-native environments that heavily rely on containerization and Kubernetes for managing resources.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/700/1*BsiwJA56KQDm2khWkdkLEQ.png\"></figure><h3>Scalability in\u00a0Airflow</h3>\n<p>The <strong>KubernetesExecutor</strong> in Airflow offers seamless scalability without requiring manual setup, as Kubernetes handles the scaling process entirely. By using this executor, you only need to focus on managing the scheduler by creating one or multiple scheduler instances based on your workload. Airflow will automatically generate separate worker pods for each task, providing a highly flexible cluster size and ensuring exceptional scalability.</p>\n<p>However, there are two significant drawbacks to consider with this executor:</p>\n<ol>\n<li>\n<strong>Resource Overhead due to Pod-per-Task Approach</strong>: One significant challenge of using Airflow with the KubernetesExecutor is the inherent resource overhead. With this executor, Airflow launches a separate pod for each task, regardless of its size or resource requirements. While this approach provides isolation and fault tolerance, it can result in inefficient utilization of resources. Small tasks that could have been executed on a shared pod end up consuming a full pod\u2019s worth of resources, leading to\u00a0wastage.</li>\n<li>\n<strong>Increased Latency in Task Execution</strong>: Another limitation of the KubernetesExecutor is the latency introduced during task execution. Since each task is assigned to a separate pod, the orchestration overhead associated with pod creation and initialization adds to the overall latency. Even for small and quick tasks, the time taken for pod startup and teardown can become a significant bottleneck.</li>\n</ol>\n<h4>What about CeleryExecutor?</h4>\n<p>Although the <strong>CeleryExecutor</strong> provides distributed task execution in Airflow, scaling the Celery cluster itself can be a challenge. However, by deploying Celery on Kubernetes and utilizing Kubernetes-based Event Driven Autoscaling (KEDA), we can overcome this limitation and achieve dynamic scaling based on specific conditions. One such condition could be the number of queued tasks in\u00a0Airflow.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/820/1*f726_zVaX7rf9KQ06HInqw.png\"><figcaption>CeleryExecutor with\u00a0KEDA</figcaption></figure><p>KEDA integrates seamlessly with Kubernetes and provides an autoscaling solution that takes into account various metrics, including external event sources, to trigger scaling actions. By combining the power of Celery, Kubernetes, and KEDA, Airflow users can achieve efficient and dynamic scaling of their CeleryExecutor clusters, ensuring optimal performance and resource utilization for their data workflows.</p>\n<p>By leveraging Kubernetes and KEDA, we can configure rules that monitor the number of queued tasks in Airflow. When the number of queued tasks exceeds a certain threshold, KEDA can automatically scale up the number of Celery workers by adding additional worker pods to the cluster. This dynamic scaling ensures that the cluster adapts to the workload demands, enabling efficient resource utilization and improved task processing times.</p>\n<p>If you are using the official helm chart to install Airflow on Kubernetes, you can easily activate KEDA by adding these configurations to the values\u00a0file:</p>\n<pre>workers:<br>  keda:<br>    enabled: true<br><br>    # How often KEDA polls the airflow DB to report new scale requests to the HPA<br>    pollingInterval: 5<br><br>    # Minimum number of workers created by keda<br>    minReplicaCount: 4<br><br>    # Maximum number of workers created by keda<br>    maxReplicaCount: 10</pre>\n<p>Airflow uses the following query in the CeleryExecutor to determine the appropriate number of workers for the\u00a0cluster:</p>\n<pre>SELECT ceil(COUNT(*)::decimal / {{ .Values.config.celery.worker_concurrency }})<br>FROM task_instance<br>WHERE (state='running' OR state='queued')</pre>\n<p>However, this approach can lead to instability in the number of workers, especially with small tasks and sensors in \u201creschedule mode\u201d. In such case, KEDA may increase the number of replicas as soon as a queued task is waiting for a worker, without waiting for the running tasks to finish. This can result in the creation and initialization of new pods, and potentially, a new node would be added to the cluster by the node auto-scaler. However, there may be a latency of 1 to 3 minutes between the scaling decision and the worker being ready to execute the tasks. In the meantime, some of the other tasks might have already finished, and the queued tasks could have been executed by other workers. As a result, KEDA might decide to reduce the number of replicas, leading to the addition of a new node that is running idle for a\u00a0while.</p>\n<h4>Better KEDA queries for some use\u00a0cases</h4>\n<p>To ensure that the current workers have a chance to execute the queued tasks, we can modify the query by adding a filter based on the queued time. This filter will consider only the running tasks and the tasks that have been queued for more than X\u00a0minutes:</p>\n<pre>SELECT ceil(COUNT(*)::decimal / {{ .Values.config.celery.worker_concurrency }})<br>FROM task_instance<br>WHERE state='running' <br>  OR (state='queued' AND queued_dttm &lt; NOW() - INTERVAL '1 minute');</pre>\n<p>By implementing this filter, we introduce a delay of X minutes in the scaling-up decision, allowing the current workers to process the queued tasks. Consequently, the overall execution time of the tasks will also include an additional X minutes in such scenarios where a scale-up is genuinely required.</p>\n<p>To prioritize critical tasks over less important ones without introducing extra latency to the critical tasks, you can use a variant of the query based on the priority_weight and queued_dttm columns (you should provide priority_weight parameter and use <a href=\"https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/priority-weight.html\">absolute weighting method</a>):</p>\n<pre>SELECT ceil(COUNT(*)::decimal / {{ .Values.config.celery.worker_concurrency }})<br>FROM task_instance<br>WHERE state = 'running'<br>  OR (<br>    state = 'queued' AND (<br>      priority_weight &gt;= 5<br>      OR queued_dttm &lt; NOW() - INTERVAL ((5 - priority_weight) || ' minute')<br>    )<br>  );</pre>\n<p>In this query, a conditional statement (priority_weight &gt;= 5) is used to determine if the dynamic interval should be applied. If the priority_weight is greater than or equal to 5 (indicating a critical task), the first condition (priority_weight &gt;= 5) is satisfied, and the second condition queued_dttm &lt; NOW() - INTERVAL '1 minute'is not considered.</p>\n<p>However, if the priority_weight is less than 5, the first condition is not satisfied, and the second condition is evaluated. In this case, the interval is calculated as (5 - priority_weight) concatenated with the string \u2018 minute\u2019. This calculation allows for some tolerance based on the task criticality.</p>\n<p>Please note that updating the KEDA query is not currently possible in the official helm chart, but it will be feasible in the next release 1.11.0 (<a href=\"https://github.com/apache/airflow/pull/32308\">link to\u00a0PR</a>).</p>\n<h4>Scaling with long-running tasks</h4>\n<p>One important consideration when using workers auto-scaling with a mix of long and short running tasks is how the scaling behavior can impact the execution of long-running tasks. Let\u2019s explore this scenario in more\u00a0details:</p>\n<p>In a situation where the tasks are scheduled hourly and comprise a mixture of long and short running tasks, the scaling behavior becomes crucial. Suppose the number of queued tasks is between 3 and 4 times the value of worker concurrency. Here\u2019s how the scaling process\u00a0unfolds:</p>\n<ol>\n<li>\n<strong>Scaling Up with KEDA</strong>: When the tasks start, KEDA helps drive the scaling up the workers to ensure all the tasks can run concurrently. In this case, KEDA might scale up the number of replicas to\u00a04.</li>\n<li>\n<strong>Scaling Down with HPA</strong>: As the short-running tasks complete, the Horizontal Pod Autoscaler (HPA) could make a decision to scale down the number of replicas from 4 to 2, as the workload decreases. However, in this scaling-down process, there is no control over which specific replicas get terminated.</li>\n<li>\n<strong>Impact on Long-Running Tasks</strong>: The scaling down of replicas can potentially impact long-running tasks that are still in progress. If a long-running task was running on one of the terminated replicas, it would be retried from the beginning on another available worker (that depends on the retry configuration of the task instance). This retry behavior occurs because the terminated replica\u2019s state is lost, and the task needs to be rescheduled on a different worker.</li>\n</ol>\n<h4>Leverage the container lifecycle</h4>\n<p>Kubernetes offers lifecycle hooks that can be utilized to delay the termination process. Consider a scenario where a worker, currently 40 minutes into executing a one-hour task, is scheduled for termination. Kubernetes sends a SIGTERM signal to indicate the intent to terminate the worker pod. However, instead of terminating immediately, the worker can receive and process this signal to delay the termination until the ongoing tasks are completed.</p>\n<p>By implementing the necessary logic within the lifecycle hooks, the worker pod can respond to the SIGTERM signal, indicating that it needs additional time to finish the current tasks. Kubernetes honors this request by waiting for either a response to the SIGTERM signal or until the specified terminationGracePeriodSeconds has elapsed before forcibly terminating the worker pod. This ensures that the ongoing tasks have an opportunity to complete before the worker pod is forcefully terminated, promoting a smoother shutdown process and minimizing disruptions.</p>\n<p>When CeleryExecutor is used in Airflow, Airflow creates Celery workers and treats them as Airflow workers, leading to handling the system signals such as SIGTERM by Celery. Here is the sequence of actions taken by the worker upon receiving a SIGTERM\u00a0signal:</p>\n<ol>\n<li>\n<strong>Unsubscribing from Queues</strong>: Upon receiving the SIGTERM signal, the worker immediately unsubscribes from all the queues it is currently listening to. This ensures that no new tasks are assigned to the worker during the termination process.</li>\n<li>\n<strong>Waiting for Task Completion</strong>: The worker then waits for all the tasks it is currently executing to finish. Despite the termination signal, the worker will continue to acknowledge the tasks as they are completed.</li>\n<li>\n<strong>Closing Connections and Exiting</strong>: Once all the tasks are complete, the worker proceeds to close all connections, stop any associated subprocesses, and then exits gracefully. This ensures that the worker terminates cleanly and releases any held resources.</li>\n</ol>\n<p>However, if the terminationGracePeriodSecondsis reached during this sequence, Kubernetes will forcibly terminate the worker. Therefore, it is essential to carefully configure this parameter based on the characteristics of your\u00a0tasks.</p>\n<p>This solution is effective for tasks with a maximum duration of one hour. Setting the terminationGracePeriodSeconds to a value greater than one hour can result in excessive resource wastage. As described in the first step of the actions sequence, the worker stops accepting new tasks upon receiving a SIGTERM signal. Therefore, configuring the terminationGracePeriodSeconds to a longer duration, such as 24 hours, would allocate the entire worker\u2019s resources to a single task, which is not desirable.</p>\n<h3>Don\u2019t worry, Airflow has the\u00a0solution</h3>\n<p>While Airflow is capable of running tasks on its workers, it primarily serves as a scheduler for managing workflows. In many cases, the heavy lifting of executing massive tasks is offloaded to external systems (Docker, Kubernetes, Spark, Trino, Athena, Snowflake,\u00a0\u2026). Airflow tasks often involve checking the state of processing on these external systems and waiting to receive the\u00a0results.</p>\n<p>To optimize resource utilization and minimize idle operators or sensors, Airflow introduced a core component called the Triggerer. The <strong>Triggerer</strong> allows lightweight checks to be performed instead of running the task or sensor continuously on a worker and occupying a worker slot. Instead, these tasks can suspend themselves when they know they need to wait and delegate the responsibility of resuming them to a\u00a0Trigger.</p>\n<p><strong>Triggers</strong> are small, asynchronous pieces of Python code designed to be executed together in a single Python process. Their asynchronous nature enables them to coexist efficiently. Here\u2019s an overview of how this process\u00a0works:</p>\n<ol>\n<li>\n<strong>Task Deferral</strong>: When a task instance reaches a point where it needs to wait, it defers itself using a trigger associated with the event that should resume it. By deferring, the worker is freed up to execute other\u00a0tasks.</li>\n<li>\n<strong>Registration and Execution</strong>: The new Trigger instance is registered within Airflow and picked up by a triggerer process, which is responsible for handling triggers.</li>\n<li>\n<strong>Trigger Execution</strong>: The triggerer process runs the trigger until it reaches the firing condition specified. Once the trigger fires, it triggers the rescheduling of the associated source\u00a0task.</li>\n<li>\n<strong>Task Resumption</strong>: The scheduler then adds the task to the queue to be executed on a worker node, ensuring its execution resumes when resources are available.</li>\n</ol>\n<p>Implementing a trigger for your operator is typically not a complex task. However, it relies on the availability of clients or libraries that facilitate asynchronous communication with the external system. The Airflow community, including committers and contributors, is actively working on expanding the support for the deferrable mode across a wide range of operators provided by the community.</p>\n<p>To illustrate how to implement a trigger in custom operators, here\u2019s an\u00a0example:</p>\n<pre>class ExampleOperator(BaseOperator):<br>  def execute(self, context):<br>    # run the task on a external system<br>    ...<br>    # then defer it to create the trigger and leave the worker slot<br>    self.defer(<br>      trigger=ExampleTrigger(),<br>      method_name=\"resume_execution\"<br>    )<br><br>  def resume_execution(self, context: dict, event: dict):<br>    # resume the execution<br><br>class ExampleTrigger(BaseTrigger):<br>  def serialize(self):<br>    return (\"package.module.ExampleTrigger\", {})<br><br>  async def run(self):<br>    step = 1 # number of seconds to wait between checks<br>    while True:<br>      # async check with the external system<br>      if not finished:<br>        await asyncio.sleep(step)<br>      else:<br>        yield TriggerEvent({<br>          # put anything you want to send to the resume_execution method<br>        })<br>        return</pre>\n<h4>Triggerers availability</h4>\n<p>Triggers are designed to be highly available, allowing us to run multiple instances of the triggerer on different hosts. These instances utilize DB locking to coexist, similar to the schedulers.</p>\n<p>Since the triggers consist of lightweight and asynchronous Python code, by default, each triggerer runs 1000 triggers simultaneously. However, we have the flexibility to increase this capacity to tens of thousands or decrease it to hundreds, depending on the workload and specific triggers.</p>\n<p>Every second, the triggerer attempts to load additional triggers, provided that its capacity allows for it. It only loads triggers that are not assigned to any triggerer or are assigned to a non-responsive one. To detect non-responsive triggerers, the triggerer retrieves the last heartbeat timestamp for each one and checks if it is older than trigger.job_heartbeat_sec (a new configuration will be introduced in Airflow 2.6.3 through this\u00a0<a href=\"https://github.com/apache/airflow/pull/32123\">PR</a>).</p>\n<p>It's important to note that a delayed heartbeat doesn't necessarily mean that the triggerer has died; it could be due to temporary network issues that prevent the heartbeats from being sent. In such cases, the same trigger might end up running on multiple triggerers simultaneously. However, this is 100% safe as long as the trigger's responsibility is respected, which involves waiting for an external task to finish. Airflow will process the first sent event to resume the task and ignore the duplicates. Therefore, it's crucial to consider that Airflow provides an at-least-once guarantee for triggers, rather than an exactly-once guarantee.</p>\n<h4>KEDA for triggerers</h4>\n<p>As we can observe, workers and triggerers share certain characteristics. Both can execute a limited number of tasks/triggers at once, and the number of instances of each service should be determined based on the workload. For that, it would be beneficial to utilize KEDA to scale the triggerers\u2019 deployment, just as we do for the\u00a0workers.</p>\n<p>However, there is a difference in behavior between the triggerers and the workers when it comes to handling killed instances. The triggerers, being fully fault-tolerant, do not require the same level of concern as the workers. In the case where KEDA decides to reduce the number of triggerers and randomly kill one, the remaining triggerers will wait for trigger.job_heartbeat_sec before assuming responsibility for the affected triggers. They will then proceed to adopt and run them, then enter the checking loop and monitor the state of the external tasks. This ensures that the triggers continue to function properly despite the intermittent loss of triggerer instances.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*t6YNd6w1dUGJ5eZyVvPdqw.png\"><figcaption>CeleryExecutor and Triggerers with\u00a0KEDA</figcaption></figure><p>In version 1.11.0 of the Airflow Helm chart (link to PR), the integration of KEDA with triggerers will be introduced (<a href=\"https://github.com/apache/airflow/pull/32302\">link to PR</a>). By default, the chart will use the following query for\u00a0KEDA:</p>\n<pre>SELECT ceil(COUNT(*)::decimal / {{ .Values.config.triggerer.default_capacity }})<br>FROM trigger</pre>\n<p>However, you will have the flexibility to provide your own query. For instance, if you want KEDA to wait for a minute before scaling up, you can return the count of assigned triggers and the count of unassigned triggers older than one minute. Here is an example of the Helm values configuration:</p>\n<pre>triggerer:<br>  enabled: true<br>  keda:<br>    enabled: true<br>    minReplicaCount: 0<br>    maxReplicaCount: 3<br>    query: &gt;-<br>      SELECT ceil(COUNT(*)::decimal / {{ .Values.config.triggerer.default_capacity }})<br>      FROM trigger<br>      WHERE triggerer_id IS NOT NULL OR created_date &lt; NOW() - INTERVAL '1 minute'</pre>\n<h4>KPO</h4>\n<p>As Airflow recommend running tasks on external systems, one such system that deserves highlighting is the Kubernetes cluster where Airflow is deployed. When dealing with tasks that require a long running time or extensive resource requirements, one of the optimal choices is to utilize the KubernetesPodOperator. This operator executes a Docker container within a Kubernetes pod, ensuring all the necessary resources are provisioned, including memory, CPU, volumes, and configurations stored in the Kubernetes database. By leveraging the KubernetesPodOperator, Airflow can efficiently manage and execute tasks with demanding resource needs within the Kubernetes cluster.</p>\n<p>While Airflow offers a custom executor that defaults to using CeleryExecutor and provides KubernetesExecutor as an on-demand option (CeleryKubernetesExecutor), my preferred combination is CeleryExecutor with KubernetesPodOperator (KPO). The integration of CeleryExecutor with KPO offers several advantages.</p>\n<p>With KPO, the created container is fully independent of Airflow, eliminating the need to install the Airflow Python library and reducing the image size. This independence simplifies testing and makes upgrades much easier. Additionally, KPO allows us the freedom to run any type of image and execute various types of\u00a0code.</p>\n<p>On the other hand, KubernetesExecutor creates an Airflow worker and is primarily focused on running Python code. It requires using the same Docker image for all tasks (or an image per group of tasks), resulting in larger image sizes, unnecessary dependencies, and complications with Airflow-related testing.</p>\n<p>This combination is particularly advantageous when running the operator in deferrable mode. The task instance simply creates the pod and defers itself, thereby delegating the responsibility of waiting to a lightweight trigger. This approach minimizes the resources required for waiting and allows for efficient utilization of the system. By using CeleryExecutor with KPO in deferrable mode, we can optimize task execution and enhance the overall performance of\u00a0Airflow.</p>\n<h3>The ideal architecture for big\u00a0projects</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*qNAVAYNkaczMNqjP95BIAA.png\"></figure><p>In summary, the <strong>CeleryExecutor</strong> stands out among other Airflow executors in terms of execution latency and resource management. Its integration with Kubernetes and KEDA allows for scalability based on workload. By setting terminationGracePeriodSeconds to 1 hour, most of the running tasks can avoid interruption when a scale-down decision is made by\u00a0KEDA.</p>\n<p>For tasks executed outside of Airflow, I recommend using the deferable mode to free up worker slots and scale down the cluster as needed. Triggers can be created to run on triggerers with low resource consumption. These triggerers are highly available and fault-tolerant. To adapt to varying workload sizes, especially in projects with hourly or daily peaks, we explored how to use KEDA to scale triggerers without worrying about their execution.</p>\n<p>Additionally, I highlight the benefits of using the KubernetesPodOperator as the preferred method for executing long-running or resource-intensive tasks. It surpasses the use of the CeleryKubernetesExecutor due to the lightweight used images, flexibility in choosing images and applications, ease of testing, and simplicity in upgrading Airflow versions.</p>\n<p>This architecture is suitable for projects of any size but requires knowledge of various technologies such as Celery, Kubernetes, and KEDA. It is particularly recommended for larger projects with intermittent peaks where reducing execution latency \u26a1\ufe0f, infrastructure costs \ud83d\udcb8, and resource consumption \ud83c\udf31are the key\u00a0goals.</p>\n<h4>Next Articles</h4>\n<p>In the upcoming articles related to the Airflow topic, we will dive deeper into various aspects of Airflow optimization and design. Here\u2019s a glimpse of what you can\u00a0expect:</p>\n<ol>\n<li>\n<strong>Optimizing Scheduler Performance</strong>: Learn valuable techniques and best practices to fine-tune the performance of the Airflow scheduler. Discover ways to optimize resource utilization and reduce scheduling latency for your workflows.</li>\n<li>\n<strong>Designing Performant DAGs</strong>: Explore strategies for creating efficient DAGs in Airflow. We\u2019ll discuss approaches to minimize redundancy, improve task parallelism, and ensure smooth execution of your workflows.</li>\n<li>\n<strong>Building Dynamic DAGs and Tasks</strong>: Harness the full power of Airflow\u2019s dynamic capabilities. Discover how to dynamically generate DAGs and tasks based on runtime conditions, enabling you to create flexible and scalable workflows.</li>\n</ol>\n<p>Stay tuned for these upcoming articles as we unlock advanced Airflow concepts and empower you to take your data orchestration to the next\u00a0level!</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=8edb4f8aed65\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/apache-airflow/airflow-scalable-and-cost-effective-architecture-8edb4f8aed65\">Airflow: Scalable and Cost-Effective Architecture</a> was originally published in <a href=\"https://medium.com/apache-airflow\">Apache Airflow</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","enclosure":{},"categories":["celery","data-engineering","kubernetes","airflow","keda"]}]}